<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.2.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"ultramarine-indigo.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.18.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="统一模态大模型通过将不同模态的数据（如文本、图像、音频等）统一转化为token序列，并利用一个共享的Transformer架构进行联合建模，简化了传统多模态模型中依赖视觉编码器提取特征的步骤。其核心优势在于消除了独立特征提取器的依赖，使得不同模态的数据能够在同一网络中进行端到端的学习，提升了跨模态的联合建模能力。这种模型结构更为简洁，但也面临高维数据处理和训练难度的挑战。">
<meta property="og:type" content="article">
<meta property="og:title" content="统一模态大模型梳理总结">
<meta property="og:url" content="http://ultramarine-indigo.github.io/2024/08/05/%E7%BB%9F%E4%B8%80%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%A2%B3%E7%90%86%E6%80%BB%E7%BB%93/index.html">
<meta property="og:site_name" content="ultramarine&#39;s Blog">
<meta property="og:description" content="统一模态大模型通过将不同模态的数据（如文本、图像、音频等）统一转化为token序列，并利用一个共享的Transformer架构进行联合建模，简化了传统多模态模型中依赖视觉编码器提取特征的步骤。其核心优势在于消除了独立特征提取器的依赖，使得不同模态的数据能够在同一网络中进行端到端的学习，提升了跨模态的联合建模能力。这种模型结构更为简洁，但也面临高维数据处理和训练难度的挑战。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/ultramarine-indigo/ultramarine-indigo.github.io/main/imgs/Unified-IO2-framework.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ultramarine-indigo/ultramarine-indigo.github.io/main/imgs/AnyGPT-framework.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ultramarine-indigo/ultramarine-indigo.github.io/main/imgs/Chameleon-framework.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ultramarine-indigo/ultramarine-indigo.github.io/main/imgs/xGen-MM(BLIP-3)-framework.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ultramarine-indigo/ultramarine-indigo.github.io/main/imgs/Transfusion-framework.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ultramarine-indigo/ultramarine-indigo.github.io/main/imgs/Transfusion-1.png">
<meta property="article:published_time" content="2024-08-05T10:58:51.000Z">
<meta property="article:modified_time" content="2024-09-10T13:02:58.959Z">
<meta property="article:author" content="Ultramarine">
<meta property="article:tag" content="多模态">
<meta property="article:tag" content="统一模态">
<meta property="article:tag" content="Transfusion">
<meta property="article:tag" content="Chameleon">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/ultramarine-indigo/ultramarine-indigo.github.io/main/imgs/Unified-IO2-framework.png">


<link rel="canonical" href="http://ultramarine-indigo.github.io/2024/08/05/%E7%BB%9F%E4%B8%80%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%A2%B3%E7%90%86%E6%80%BB%E7%BB%93/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://ultramarine-indigo.github.io/2024/08/05/%E7%BB%9F%E4%B8%80%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%A2%B3%E7%90%86%E6%80%BB%E7%BB%93/","path":"2024/08/05/统一模态大模型梳理总结/","title":"统一模态大模型梳理总结"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>统一模态大模型梳理总结 | ultramarine's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">ultramarine's Blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section">首页</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section">关于</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section">标签</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section">分类</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section">归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#data-perspective"><span class="nav-text">Data Perspective</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#visionx"><span class="nav-text">Vision+X</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3"><span class="nav-text">基本思想</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%A8%A1%E6%80%81%E6%95%B0%E6%8D%AE"><span class="nav-text">模态数据</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#vision"><span class="nav-text">vision</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#audio"><span class="nav-text">audio</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#text"><span class="nav-text">Text</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-text">总结</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#unified-multimodal-large-model"><span class="nav-text">Unified Multimodal Large
Model</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#llamagen"><span class="nav-text">LlamaGen</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3-1"><span class="nav-text">基本思想</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%85%B7%E4%BD%93%E6%96%B9%E6%B3%95"><span class="nav-text">具体方法</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#unified-io-2"><span class="nav-text">Unified-IO 2</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3-2"><span class="nav-text">基本思想</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%85%B7%E4%BD%93%E6%96%B9%E6%B3%95-1"><span class="nav-text">具体方法</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E7%BB%9F%E4%B8%80%E8%A1%A8%E7%A4%BA"><span class="nav-text">统一表示</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E6%9E%B6%E6%9E%84"><span class="nav-text">架构</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E7%9B%AE%E6%A0%87"><span class="nav-text">训练目标</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#anygpt"><span class="nav-text">AnyGPT</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3-3"><span class="nav-text">基本思想</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%85%B7%E4%BD%93%E6%96%B9%E6%B3%95-2"><span class="nav-text">具体方法</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%A4%9A%E6%A8%A1%E6%80%81%E7%BC%96%E7%A0%81%E5%99%A8tokenization"><span class="nav-text">多模态编码器（Tokenization）</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#llm"><span class="nav-text">LLM</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#multimodal-generation"><span class="nav-text">Multimodal
Generation</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B"><span class="nav-text">训练流程</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#chameleon"><span class="nav-text">Chameleon</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3-4"><span class="nav-text">基本思想</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%85%B7%E4%BD%93%E6%96%B9%E6%B3%95-3"><span class="nav-text">具体方法</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#tokenization"><span class="nav-text">Tokenization</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#pre-training"><span class="nav-text">Pre-Training</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#stablity"><span class="nav-text">stablity</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#xgen-mmblip-3"><span class="nav-text">xGEN-MM(BLIP-3)</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3-5"><span class="nav-text">基本思想</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84"><span class="nav-text">模型结构</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83"><span class="nav-text">训练</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#transfusion"><span class="nav-text">Transfusion</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3-6"><span class="nav-text">基本思想</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84-1"><span class="nav-text">模型结构</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93-1"><span class="nav-text">总结</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BB%9F%E4%B8%80%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B"><span class="nav-text">统一模态大模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#tokenization-1"><span class="nav-text">tokenization</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#tokenization%E8%AE%BE%E8%AE%A1"><span class="nav-text">tokenization设计</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#image-tokenization%E7%9A%84%E7%B2%92%E5%BA%A6"><span class="nav-text">image tokenization的粒度</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#training"><span class="nav-text">Training</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#scaling"><span class="nav-text">Scaling</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Ultramarine"
      src="/images/header.jpg">
  <p class="site-author-name" itemprop="name">Ultramarine</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives">
          <span class="site-state-item-count">22</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags">
        <span class="site-state-item-count">41</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://ultramarine-indigo.github.io/2024/08/05/%E7%BB%9F%E4%B8%80%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%A2%B3%E7%90%86%E6%80%BB%E7%BB%93/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/header.jpg">
      <meta itemprop="name" content="Ultramarine">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ultramarine's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="统一模态大模型梳理总结 | ultramarine's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          统一模态大模型梳理总结
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-08-05 18:58:51" itemprop="dateCreated datePublished" datetime="2024-08-05T18:58:51+08:00">2024-08-05</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AE%BA%E6%96%87%E8%AF%A6%E8%A7%A3/" itemprop="url" rel="index"><span itemprop="name">论文详解</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>统一模态大模型通过将不同模态的数据（如文本、图像、音频等）统一转化为token序列，并利用一个共享的Transformer架构进行联合建模，简化了传统多模态模型中依赖视觉编码器提取特征的步骤。其核心优势在于消除了独立特征提取器的依赖，使得不同模态的数据能够在同一网络中进行端到端的学习，提升了跨模态的联合建模能力。这种模型结构更为简洁，但也面临高维数据处理和训练难度的挑战。</p>
<span id="more"></span>
<h3 id="data-perspective">Data Perspective</h3>
<h4 id="visionx">Vision+X</h4>
<p>论文：Vision + X: A Survey on Multimodal Learning in the Light of
Data (TPAMI 2024) <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2210.02884">paper</a></p>
<h5 id="基本思想">基本思想</h5>
<p>从不同数据模态的内在性质对多模态机器学习进行了调查，首先分析了每种数据格式的通用性和唯一性，主要来自视觉、音频、文本和动作，然后介绍了由数据模态（如
Vision+Text）组合的方法进步。一方面，强调和利用特定数据模态的独特特征将有助于解决与这些模态相关的具体应用问题。另一方面，识别不同模态之间的共性能帮助构建一个更统一和协作的框架，该框架反映了真实人类智能系统的能力。希望探索对齐以及数据模态的内在性质与技术设计之间的差异，以更好地解决与具体多模态任务相关的特定挑战，促使统一的多模态机器学习框架。</p>
<h5 id="模态数据">模态数据</h5>
<h6 id="vision">vision</h6>
<p>视觉数据通常被认为是高维的“原始数据”。它包含丰富的特征和细节，代表丰富的视觉内容。然而，连续空间和时间方面的冗余给多模态学习任务中的处理、分析和有效利用带来了挑战。</p>
<ul>
<li>image：特点是对变换的固有不变性。</li>
<li>video：与静态图像不同，视频在时间维度上封装了信息。通常需要对时间元素(如动作、运动、光流)进行额外的理解和分析。</li>
</ul>
<h6 id="audio">audio</h6>
<p>音频是直接从环境中捕获的“原始数据”形式，在时间维度上具有固有的连续性。最常见的基于非学习的音频连续数据格式是波形。波形是一个二维数据，描述了空气振动在时域测量的声压变化。另一种通用的音频表示类型是频谱图。与强调音频信号的时间变化的波形相比，谱图还反映了声音随时间的频率内容。</p>
<ul>
<li>music：除了时间维度外，音频数据通常以频谱图表示的形式来表征频率特征。除了时间相干性外，节奏是一个重要独特音乐特征。</li>
<li>speech：语音音频的一个明显独特性在于它与语言的自然相关性，其中语音音频的离散表示与语言标记对齐。</li>
<li>Ambient
Sound：与音乐和语音音频信号相比，环境声音表现出更嘈杂的性质。环境声音的表示更加模糊，缺乏明确的格式。</li>
</ul>
<h6 id="text">Text</h6>
<p>与可以被视为“原始数据”的视觉和音频信息不同，文本数据经过大量处理。尽管语言差异，但格式高度统一，语义精确。这意味着文本信息量大、紧凑，而视觉和音频信号通常包含丰富的信息冗余。</p>
<h5 id="总结">总结</h5>
<p>视觉数据和几种类型的音频数据（环境音），可以被视为原始信息源。这些模式包括直接从通常高维的环境中捕获的感官信息，可以通过信息冗余进一步处理和分析。文本数据和某些类型的音频信号（例如语音）相比，这些数据模式已经拥有有意义的语义，并且以令牌的形式通过统一的表示更紧凑。</p>
<p>数据性质的区别，特别是在语义方面，在塑造各自研究领域的方法和技术进步方面发挥了重要作用。</p>
<ul>
<li>文本数据的高度处理性质及其一致的问题公式为开发大规模基础模型铺平了道路，并在广泛的
NLP
任务中表现出卓越的性能。文本数据的统一性质允许这些模型在不进行重大修改的情况下应用于各种任务，利用语义丰富性和一致的问题表述。</li>
<li>视觉数据是原始信息源，需要大量的表示学习和特定的下游应用程序阶段来获得有效和处理的可视化表示。视觉数据的复杂性和视觉任务的多样性使得开发一个可以应用于棋盘的统一基础模型更具挑战性。</li>
</ul>
<p>数据格式，无论是连续还是离散的，在确定合适的模型架构以进行有效处理中起着至关重要的作用。</p>
<ul>
<li>对于图像和环境声音等连续数据，空间和时间维度的连续性通常受益于模型架构，如专门用于处理空间和时间依赖性的
CNN，并捕获数据中的局部和全局相关性。</li>
<li>至于具有离散格式的数据，例如文本单词标记，像 Transformers
这样的模型更适合对离散元素之间的依赖关系进行建模。</li>
</ul>
<h3 id="unified-multimodal-large-model">Unified Multimodal Large
Model</h3>
<h4 id="llamagen">LlamaGen</h4>
<p>论文：Autoregressive Model Beats Diffusion: Llama for Scalable Image
Generation <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.06525">paper</a></p>
<h5 id="基本思想-1">基本思想</h5>
<p>基于自回归的LLM通过 “next-token
prediction”的形式展现了强大的能力，本文试图将大型语言模型的“next-token
prediction”范式应用于视觉生成领域。以下三个问题是模型设计的关键。</p>
<ul>
<li>设计良好的图像压缩器</li>
<li>可扩展的图像生成模型</li>
<li>高质量的训练数据</li>
</ul>
<h5 id="具体方法">具体方法</h5>
<p><strong>Image
Tokenizer</strong>：采用类似于VQGAN的量化自动编码器（quantized
autoencoder）架构，使用卷积网络（ConvNet）作为编码器和解码器，并通过代码表（codebook）进行特征量化。这个分词器将图像转化为离散token，适用于自回归模型的生成过程。其中，对码本向量使用ℓ2-归一化，采用较低的码本向量维度C和较大的码本大小K。分词器的设计重点在于高重构质量，即从token还原回来的图像应尽可能接近原始图像。</p>
<p><strong>loss设计</strong>：</p>
<ul>
<li>对于VQ学习，包含两个L2 loss
分别训练Encoder和codebook。由于量化无法梯度回传，分别将两个特征的梯度终止，码本梯度终止时使码本向量接近encoder提取的特征向量；encoder梯度终止时使encoder特征向量接近码本向量。</li>
<li>对于图像重建，结合重构损失（reconstruction
loss）、感知损失（perceptual loss）和对抗性损失（adversarial
loss）来优化模型。</li>
</ul>
<p><strong>architecture</strong>：Llama架构在这里被用于图像生成。该架构采用预归一化（pre-normalization）、SwiGLU激活（SwiGLU
activation）和旋转位置嵌入（rotary positional embeddings）</p>
<ul>
<li><strong>Class-conditional image
generation</strong>：类别嵌入从一组可学习的嵌入中索引，并用作预填充的令牌嵌入。</li>
<li><strong>Text-conditional image generation</strong>：使用FLAN-T5
XL（Chung et al.
2024）作为文本编码器，编码的文本特征通过额外的MLP投影得到文本token。</li>
<li><strong>Classifier-free
guidance</strong>：条件被随机删除并替换为一个空的无条件嵌入。</li>
</ul>
<p>数据集：LAION-COCO</p>
<h4 id="unified-io-2">Unified-IO 2</h4>
<p>论文：Unified-IO 2: Scaling Autoregressive Multimodal Models with
Vision Language Audio and Action (CVPR2024) <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2024/html/Lu_Unified-IO_2_Scaling_Autoregressive_Multimodal_Models_with_Vision_Language_Audio_CVPR_2024_paper.html">paper</a></p>
<h5 id="基本思想-2">基本思想</h5>
<p>心理学认为感知系统的冗余是相互改进的监督机制，这为创建具有类似学习能力的模型提供了自然的动机，支持多种不同的模态（如视觉、听觉等），这些模态在训练过程中可以相互监督。Unified-IO
2是第一个能够理解和生成图像、文本、音频和动作的自回归多模态模型。为了统一不同的模式，将输入和输出（图像、文本、音频、动作、边界框等）标记为共享语义空间，然后使用单个encoder-decoder
transformer
模型对其进行处理。由于使用这种不同模式的训练具有挑战性，本文提出了各种架构改进来稳定模型训练，并利用多模态混合去噪目标来进行训练。</p>
<h5 id="具体方法-1">具体方法</h5>
<p><img src="https://raw.githubusercontent.com/ultramarine-indigo/ultramarine-indigo.github.io/main/imgs/Unified-IO2-framework.png" alt="Chameleon-framework" style="zoom:40%;"></p>
<h6 id="统一表示">统一表示</h6>
<ul>
<li><strong>文本、稀疏结构</strong>：文本使用来自
LLAMA的BPE编码。对边界框、关键点和相机姿势等稀疏结构进行离散化，然后使用添加到词汇表的
1000 个特殊标记进行编码。</li>
<li><strong>图像、密集结构</strong>：图像使用预训练的ViT进行编码。ViT
的第二层和倒数第二层的特征连接起来经过线性层，以捕获低级和高级视觉信息。对应生成图像，使用VQGAN将图像转换为离散标记，这些标记被添加到词汇表中，然后用作目标输出序列以生成图像。</li>
<li><strong>语音</strong>：将音频编码为频谱图，然后使用预训练的 Audio
Spectrogram Transformer (AST)对频谱图进行编码，并通过连接来自 AST
的第二层和倒数第二层特征并应用线性层来构建输入padding。对于生成，利用VIT-VQGAN将语音转换为离散token。</li>
<li><strong>图像、语音历史信息</strong>：先利用ViT/AST进行编码，然后利用perceiver
resampler将特征进一步压缩为较少数量的token（图像32，语音16）。</li>
</ul>
<h6 id="架构">架构</h6>
<p>使用encoder-decoder的Transformer架构，并设计了一些方法来解决训练不稳定的问题。在pre-training时冻结ViT和AST，并在指令微调阶段fine-tune。</p>
<ul>
<li><strong>2D Rotary Embedding</strong>：将 RoPE 扩展到二维：对于任何
2D 索引 (i, j)，将 Transformer
注意力头的每个query和key分开，并将由两个坐标中的每一个构建的单独旋转嵌入分别应用</li>
<li><strong>QK Normalization</strong>：LayerNorm应用于query和key</li>
<li><strong>Scaled Cosine Attention</strong></li>
</ul>
<h6 id="训练目标">训练目标</h6>
<p>Mixture of Denoisers (MoD)合并了span corruption 和 causal language
modeling这两个训练目标。首次启发，本文设计了Multimodal Mixture of
Denoisers。对应文本，遵循MOD包含三种standard span corruption, causal
language modeling 和 extreme span corruption。对应图像和语音，使用masked
重建和跨模态生成。</p>
<h4 id="anygpt">AnyGPT</h4>
<p>论文：Anygpt: Unified multimodal llm with discrete sequence modeling
<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2402.12226">paper</a></p>
<h5 id="基本思想-3">基本思想</h5>
<p>现有的多模态生成模型要么缺少强大而鲁棒的LLM，要么利用预训练的单模态编码器导致LLM的输入输出不一致阻碍模型训练和推理。AnyGPT利用离散表示来统一处理各种模态，包括语音、文本、图像和音乐。完全依赖于数据级别的预处理，促进新模态无缝集成到
LLM
中，类似于新语言的合并。同时，本文构建了一个多模态文本中心的数据集，用于多模态对齐预训练。</p>
<h5 id="具体方法-2">具体方法</h5>
<p><img src="https://raw.githubusercontent.com/ultramarine-indigo/ultramarine-indigo.github.io/main/imgs/AnyGPT-framework.png" alt="AnyGPT-framework" style="zoom:50%;"></p>
<p>AnyGPT模型由三个主要部分组成：多模态编码器、多模态语言模型和多模态解码器。多模态编码器将不同模态的数据（如图像、语音）转化为离散的语义符号。多模态语言模型作为核心部分，对编码后的语义符号进行统一建模，实现感知、理解、推理和生成等任务。多模态解码器则将语义符号解码回原始的模态表示。</p>
<h6 id="多模态编码器tokenization">多模态编码器（<strong>Tokenization</strong>）</h6>
<ul>
<li>image：使用SEED编码器，由ViT编码器、Causal
Q-Former、VQ码本、MLP和UNet解码器组成。输入为224x224的RGB图像，输出32个语义符号。</li>
<li>speech：采用SpeechTokenizer，这是一个基于RVQ-VAE结构的编码器-解码器。将语音序列编码为8层1024个符号的矩阵，每层捕获不同的语义和声学信息。</li>
<li>music：使用Encodec，这是一个基于卷积自编码器和RVQ的编码器。它处理32kHz单声道音频，输出250x4的矩阵，矩阵被展平成250个符号的序列。</li>
</ul>
<h6 id="llm">LLM</h6>
<p>AnyGPT使用LLaMA-2
7B作为多模态语言模型的主干，其通过扩展词汇表来集成不同模态的离散表示。AnyGPT将所有模态的符号组合成一个新词汇表，词汇表大小为所有模态词汇量之和。该模型通过next
token prediction任务对多模态符号序列进行训练。</p>
<h6 id="multimodal-generation"><strong>Multimodal
Generation</strong></h6>
<p>AnyGPT采用两级框架来生成高质量的图像、语音和音乐。首先，语言模型生成语义层面的内容，然后使用不同的非自回归模型来生成感知层面的高质量内容。</p>
<ul>
<li>图像：使用Diffusion Model从SEED语义符号生成高质量图像。</li>
<li>语音：使用SoundStorm从语义符号生成声学符号，然后通过SpeechTokenizer的解码器转换成语音波形。</li>
<li>音乐：使用Encodec的解码器从音乐符号生成高保真音频。</li>
</ul>
<h5 id="训练流程">训练流程</h5>
<p>数据集：使用了大规模的预训练数据集，其中包括图像-文本对、语音-文本对和音乐-文本对。</p>
<ul>
<li>图像-文本：使用LAION-2B、LAION-COCO、LAION-Aesthetics、JourneyDB等数据集。</li>
<li>语音-文本：使用Gigaspeech、Common Voice、Multilingual
LibriSpeech等数据集。</li>
<li>音乐-文本：从YouTube爬取100万个音乐视频，利用Spotify
API进行匹配，使用GPT-4提取摘要。</li>
</ul>
<p>此外，AnyGPT还构建了一个包含108k个样本的多模态交互数据集AnyInstruct-108k，用于微调。</p>
<h4 id="chameleon">Chameleon</h4>
<p>论文：Chameleon: Mixed-Modal Early-Fusion Foundation Models <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.09818">paper</a></p>
<h5 id="基本思想-4">基本思想</h5>
<p>当前多模态模型分别建模不同的模式，通常使用特定于模态的编码器或解码器，这可能会限制其跨模态整合信息的能力。本文提出的Chameleon是基于早期融合标记的混合模态模型，能够理解和生成任意序列中的图像和文本。通过将图像量化为离散标记，从而将相同的Transformer应用于图像和文本标记序列，而无需单独的图像/文本编码器。这种早期融合方法，其中所有模态都从一开始就投影到共享表示空间中，允许跨模式无缝推理和生成。然而，它在优化稳定性和scaling方面存在挑战。</p>
<h5 id="具体方法-3">具体方法</h5>
<p><img src="https://raw.githubusercontent.com/ultramarine-indigo/ultramarine-indigo.github.io/main/imgs/Chameleon-framework.png" alt="Transfusion-1" style="zoom:40%;"></p>
<h6 id="tokenization"><strong>Tokenization</strong></h6>
<ul>
<li>image：基于make a scene训练一个image tokenization，将512 ×
512的图像编码为1024个离散token，token来自大小为8192的码本。在预训练期间对人脸的图像百分比进行了
2 次上采样。其核心弱点是重建具有大量文本的图像，即繁重的 OCR
相关任务。</li>
<li>Tokenizer：在sentencepiece library上训练一个BPE Tokenizer。</li>
</ul>
<h6 id="pre-training"><strong>Pre-Training</strong></h6>
<p><strong>First stage</strong>：完全无监督的混合数据集</p>
<ul>
<li>Text-only：2.9万亿个text-only tokens</li>
<li>Text-Image：将图像大小调整并居中裁剪为 512 × 512
以进行标记化。包含了140亿个文本图像对，产生了1.5万亿个文本图像tokens。</li>
<li>Text/Image Interleaved：类似OBELICS，400
亿个交错文本和图像数据tokens</li>
</ul>
<p><strong>Second stage</strong>：将第一阶段数据的权重降低了
50%，并混合更高质量的数据集，同时保持图像文本token相似比例。</p>
<h6 id="stablity">stablity</h6>
<p>架构：遵循 LLAMa-2 。归一化使用 RMSNorm；使用
SwiGLU激活函数和旋转位置嵌入 (RoPE)</p>
<p>在模型扩展到超过8B参数和1T
token时，训练后期会产生明显的不稳定问题。由于所有模态共享模型权重，每个模态会尝试通过稍微增加其范数来在训练过程中“竞争”，以便在共享的权重下更好地表示自己的数据。随着训练进行，这种范数的增加会导致发散问题，特别是在超过
bf16 有效表示范围时。</p>
<p>研究人员将其归因于softmax函数所具有的平移不变性，这种现象在单模态模型中也被称为「logit
漂移」（logit
drift）。因此，论文提出了一些架构调整和优化方法来保证稳定性：</p>
<ol type="1">
<li>QK归一化（query-key normalization）：将layer
norm应用于注意力模块中的query和key向量，从而直接控制softmax层输入的norm增长。</li>
<li>在注意力层和前馈层之后引入dropout。</li>
<li>在损失函数中使用Zloss正则化。</li>
</ol>
<h4 id="xgen-mmblip-3">xGEN-MM(BLIP-3)</h4>
<p>论文：xGen-MM (BLIP-3): A Family of Open Large Multimodal Models <a target="_blank" rel="noopener" href="https://www.arxiv.org/abs/2408.08872">paper</a></p>
<h5 id="基本思想-5">基本思想</h5>
<p>xGen-MM（BLIP-3），这是一个开发大型多模态模型 (LMM)
的框架。该框架包括数据集、训练策略、模型架构和由此产生的 LMM
组件。BLIP-3相比于BLIP-2的改进包括：(1)增加训练数据的丰富性、规模和多样性，(2)用更具可扩展性的视觉令牌采样器替换Q-Former层，(3)通过在每个训练阶段将训练目标统一为单个损失来简化训练过程。</p>
<h5 id="模型结构">模型结构</h5>
<p><img src="https://raw.githubusercontent.com/ultramarine-indigo/ultramarine-indigo.github.io/main/imgs/xGen-MM(BLIP-3)-framework.png" alt="xGen-MM(BLIP-3)-framework" style="zoom:50%;"></p>
<p>xGEN-MM包括了一个<strong>ViT</strong>，<strong>vision token
sampler（perceiver
resampler）</strong>和一个预训练的<strong>LLM</strong>。</p>
<p><strong>动态高分辨率图像编码策略</strong>：在VL连接器中，使用感知器重采样器对视觉标记进行下采样。通过将单个图像分割成多个patch并分别对其进行编码，尽可能地保留原始图像的分辨率，然后将编码的图像补丁与提供全局信息的缩小原始图像连接起来。</p>
<h5 id="训练">训练</h5>
<p><strong>Pre-training</strong>：next token prediction</p>
<p><strong>Supervised Fine-tuning
(SFT)</strong>：在公共可用的指令跟踪数据集微调，使模型更好地理解并遵循用户查询。</p>
<p><strong>Interleaved Multi-Image Supervised
Fine-tuning</strong>：增强模型理解交错图像-文本输入的能力，这有助于多模态上下文学习、多图像问答。</p>
<p><strong>Post-training</strong>：首先执行DPO以提高模型的有用性和视觉忠实度，然后执行安全微调以提高模型的无害性。</p>
<h4 id="transfusion">Transfusion</h4>
<p>论文：Transfusion: Predict the Next Token and Diffuse Images with One
Multi-Modal Model <a target="_blank" rel="noopener" href="https://www.arxiv.org/abs/2408.11039">paper</a></p>
<h5 id="基本思想-6">基本思想</h5>
<p>Transfusion 是一种在离散和连续数据上训练多模态模型的方法。Transfusion
将语言建模损失函数（next token
prediction）与扩散模型相结合，在混合模态序列上训练单个转换器。我们在文本和图像的混合数据上从头开始预训练，并根据各种单模态和跨模态基准建立缩放定律。实验表明，Transfusion
缩放明显优于量化图像并在离散图像标记上训练语言模型。通过引入特定于模态的编码和解码层，可以进一步提高
Transfusion 模型的性能，甚至将每个图像压缩到只有 16
个补丁。我们进一步证明，将我们的 Transfusion 配方扩展到 7B 参数和 2T
多模态标记会产生一个模型，该模型可以以与相似的尺度扩散模型和语言模型相媲美生成图像和文本，从而获得了两个世界的好处。</p>
<h5 id="模型结构-1">模型结构</h5>
<p><img src="https://raw.githubusercontent.com/ultramarine-indigo/ultramarine-indigo.github.io/main/imgs/Transfusion-framework.png" alt="Unified-IO2-framework" style="zoom:50%;"></p>
<ul>
<li><p><strong>数据表示</strong>：文本字符串tokenize为来自固定词汇表的离散token序列，其中每个标记表示为一个整数。图像都使用
VAE 编码为latent
patches，其中每个patch表示为一个连续向量，并按从左到右从上到下顺序排序，得到图像的patch向量序列。对于混合模态，在图像特征序列前后分别添加[BOI]
和 [EOI] token，然后将其插入文本序列中。</p></li>
<li><p><strong>模型结构</strong>：模型主体为Transformer，输入输出均为高维向量。为了数据转换到同一空间，使用具有非共享参数的轻量级模态特定组件。文本利用<strong>嵌入矩阵</strong>，将每个输入整数转换为向量空间，每个输出向量转换为词汇表上的离散分布。对于图像，尝试了两种将
k × k
补丁向量的局部窗口压缩为单个向量的方案：1）线性层；2）U-net结构。</p>
<p><img src="https://raw.githubusercontent.com/ultramarine-indigo/ultramarine-indigo.github.io/main/imgs/Transfusion-1.png" alt="Transfusion-framework" style="zoom:50%;"></p></li>
<li><p><strong>Transfusion Attention</strong>：语言模型通常使用causal
mask在单个前向后向传递中有效地计算整个序列的损失和梯度，而图像通常使用不受限制的（双向）注意力进行建模。Transfusion
通过将因果注意力应用于序列中的每个元素以及在每个单独图像元素之间使用双向注意力来结合这两种模式。这允许每个图像patch关注同一图像中的所有其他patch，但只关注序列中先前出现的其他图像或文本的patch。此外，<strong>发现启用图像内注意力可以显着提高模型性能</strong>。</p></li>
<li><p><strong>训练目标</strong>：将语言建模目标LLM应用于文本标记的预测，将扩散目标LDDPM应用于图像补丁的预测。LM损失是每个token计算的，而扩散损失是每幅图像计算的，这可能会跨越序列中的多个元素。更广泛的想法是<strong>将离散分布损失与连续分布损失相结合以优化相同的模型</strong>。</p></li>
<li><p><strong>推理</strong>：推理具有两种模式，在 LM
模式下，从预测分布中逐个令牌采样，当我们采样 BOI
令牌时，解码算法切换到扩散模式。一旦扩散过程结束，我们将 EOI
标记附加到预测图像上，并转换回 LM 模式。</p></li>
</ul>
<h3 id="总结-1">总结</h3>
<h4 id="统一模态大模型">统一模态大模型</h4>
<p><strong>模型结构</strong>：token化+自回归模型 (scaling law)</p>
<h4 id="tokenization-1">tokenization</h4>
<h5 id="tokenization设计">tokenization设计</h5>
<ul>
<li><strong>VQ free</strong></li>
<li><strong>VQ based</strong></li>
</ul>
<h5 id="image-tokenization的粒度">image tokenization的粒度</h5>
<table>
<colgroup>
<col style="width: 38%">
<col style="width: 16%">
<col style="width: 26%">
<col style="width: 17%">
</colgroup>
<thead>
<tr class="header">
<th>image-level</th>
<th>pixel-level</th>
<th>patch-level</th>
<th>object-level</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>简单便于实现</td>
<td>表征细致</td>
<td>计算量小，效果较好</td>
<td>语义明确</td>
</tr>
<tr class="even">
<td>无法充分表示图片的全部信息</td>
<td>计算量巨大</td>
<td>划分固定不够灵活</td>
<td>实现流程繁琐</td>
</tr>
<tr class="odd">
<td>CLIP</td>
<td>imageGPT</td>
<td>ViT</td>
<td>SeqAE</td>
</tr>
</tbody>
</table>
<h4 id="training">Training</h4>
<ul>
<li><p>模型结构：Transformer模型(理解、离散生成)、diffusion模型（连续生成）</p></li>
<li><p>stablity：<strong>QK归一化</strong>（将layer
norm应用于注意力模块中的query和key）；在注意力层和前馈层之后引入dropout；在损失函数中使用<strong>Zloss正则化</strong></p></li>
<li><p>training stage：高质量混合数据集、多阶段pre-training、Supervised
Fine-tuning (SFT)</p></li>
<li><p>视觉问答：<strong>单模态交互（SFT）</strong>、<strong>双向多模态交互（BMT）</strong>、<strong>无监督微调（UFT）</strong></p></li>
</ul>
<h4 id="scaling">Scaling</h4>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/" rel="tag"># 多模态</a>
              <a href="/tags/%E7%BB%9F%E4%B8%80%E6%A8%A1%E6%80%81/" rel="tag"># 统一模态</a>
              <a href="/tags/Transfusion/" rel="tag"># Transfusion</a>
              <a href="/tags/Chameleon/" rel="tag"># Chameleon</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/06/16/%E6%8E%A9%E7%A0%81%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-Masked-Self-supervised-Learning-%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/" rel="prev" title="掩码自监督学习(Masked Self-supervised Learning)论文总结">
                  <i class="fa fa-angle-left"></i> 掩码自监督学习(Masked Self-supervised Learning)论文总结
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Ultramarine</span>
  </div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script size="300" alpha="0.6" zIndex="-1" src="https://cdnjs.cloudflare.com/ajax/libs/ribbon.js/1.0.2/ribbon.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
