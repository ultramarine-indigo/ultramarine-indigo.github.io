<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.2.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"ultramarine-indigo.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.18.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="语音表示学习就是将一段语音喂给自监督学习模型（SSL model），去抽一些好用的特征表示representation，这些特征再喂给Downstream models，就可以做语音识别或说话人识别等下游任务。本文将详细介绍应用于语音表示学习的一系列语音自监督模型和语音-文本多模态模型。">
<meta property="og:type" content="article">
<meta property="og:title" content="speech model语音模型总结">
<meta property="og:url" content="http://ultramarine-indigo.github.io/2023/09/13/speech-model%E8%AF%AD%E9%9F%B3%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93/index.html">
<meta property="og:site_name" content="ultramarine&#39;s Blog">
<meta property="og:description" content="语音表示学习就是将一段语音喂给自监督学习模型（SSL model），去抽一些好用的特征表示representation，这些特征再喂给Downstream models，就可以做语音识别或说话人识别等下游任务。本文将详细介绍应用于语音表示学习的一系列语音自监督模型和语音-文本多模态模型。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/ultramarine-indigo/ultramarine-indigo.github.io/main/imgs/CPC framework.png">
<meta property="article:published_time" content="2023-09-13T03:01:48.000Z">
<meta property="article:modified_time" content="2023-11-30T10:31:21.624Z">
<meta property="article:author" content="Ultramarine">
<meta property="article:tag" content="多模态">
<meta property="article:tag" content="speech model">
<meta property="article:tag" content="Self-Supervised Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/ultramarine-indigo/ultramarine-indigo.github.io/main/imgs/CPC framework.png">


<link rel="canonical" href="http://ultramarine-indigo.github.io/2023/09/13/speech-model%E8%AF%AD%E9%9F%B3%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://ultramarine-indigo.github.io/2023/09/13/speech-model%E8%AF%AD%E9%9F%B3%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93/","path":"2023/09/13/speech-model语音模型总结/","title":"speech model语音模型总结"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>speech model语音模型总结 | ultramarine's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">ultramarine's Blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section">首页</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section">关于</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section">标签</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section">分类</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section">归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#speech-ssl-model-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B"><span class="nav-text">speech SSL model
自监督学习模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#contrastive-models-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B"><span class="nav-text">Contrastive Models
对比学习模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#cpc-contrastive-predition-coding"><span class="nav-text">1. CPC Contrastive predition
Coding</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3"><span class="nav-text">基本思想</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84"><span class="nav-text">模型结构</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#wav2vec-2.0"><span class="nav-text">2. wav2vec 2.0</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3-1"><span class="nav-text">基本思想</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84-1"><span class="nav-text">模型结构</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#predictive-models-%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B"><span class="nav-text">Predictive models 预测模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#hubert"><span class="nav-text">1. HuBERT</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3-2"><span class="nav-text">基本思想</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84-2"><span class="nav-text">模型结构</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#wavlm"><span class="nav-text">2. WavLM</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3-3"><span class="nav-text">基本思想</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84-3"><span class="nav-text">模型结构</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#speech-ssl-model-%E5%B0%8F%E7%BB%93"><span class="nav-text">speech SSL model 小结</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#contrastive-models-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B-1"><span class="nav-text">Contrastive Models
对比学习模型</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#predictive-models-%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B-1"><span class="nav-text">Predictive models 预测模型</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#audio-text-multimodal-model-%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B"><span class="nav-text">audio-text multimodal
model 多模态模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#clap"><span class="nav-text">1. CLAP</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3-4"><span class="nav-text">基本思想</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84-4"><span class="nav-text">模型结构</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#flap"><span class="nav-text">2. FLAP</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3-5"><span class="nav-text">基本思想</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84-5"><span class="nav-text">模型结构</span></a></li></ol></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Ultramarine"
      src="/images/header.jpg">
  <p class="site-author-name" itemprop="name">Ultramarine</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives">
          <span class="site-state-item-count">12</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags">
        <span class="site-state-item-count">18</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://ultramarine-indigo.github.io/2023/09/13/speech-model%E8%AF%AD%E9%9F%B3%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/header.jpg">
      <meta itemprop="name" content="Ultramarine">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ultramarine's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="speech model语音模型总结 | ultramarine's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          speech model语音模型总结
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-09-13 11:01:48" itemprop="dateCreated datePublished" datetime="2023-09-13T11:01:48+08:00">2023-09-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AE%BA%E6%96%87%E8%AF%A6%E8%A7%A3/" itemprop="url" rel="index"><span itemprop="name">论文详解</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>语音表示学习就是将一段语音喂给自监督学习模型（SSL
model），去抽一些好用的特征表示representation，这些特征再喂给Downstream
models，就可以做语音识别或说话人识别等下游任务。本文将详细介绍应用于语音表示学习的一系列语音自监督模型和语音-文本多模态模型。</p>
<span id="more"></span>
<h2 id="speech-ssl-model-自监督学习模型">speech SSL model
自监督学习模型</h2>
<p>speech SSL model发展总览</p>
<figure>
<img
src="https://raw.githubusercontent.com/ultramarine-indigo/ultramarine-indigo.github.io/main/imgs/speech%20model%20list.png"
alt="speech model list" />
<figcaption aria-hidden="true">speech model list</figcaption>
</figure>
<h3 id="contrastive-models-对比学习模型">Contrastive Models
对比学习模型</h3>
<p>有一张猫图，再找一张猫（Positive pair）和一张狗（Negative
pair）的图片，把它们输入model，对于输出的特征向量，我们希望两张猫图的向量越接近越好，而我的猫图和狗图的向量值差距越大越好。这样我就训练得到一个model能够获得原始数据好的表征representation，然后做一些下游任务的时候，比如猫狗分类任务，就只需要训一个线性模型就可以很容易把它们分开。</p>
<h4 id="cpc-contrastive-predition-coding">1. CPC Contrastive predition
Coding</h4>
<p>论文：Representation Learning with Contrastive Predictive Coding <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1807.03748">paper</a></p>
<h5 id="基本思想">基本思想</h5>
<p>学习表示，以编码（高维）信号不同部分之间的基础共享信息。同时，它舍弃了更本地的低级信息和噪音。在时间序列和高维建模中，使用下一步预测的方法利用了信号的局部平滑性。当预测更远的未来时，共享信息的数量变得更低，模型需要推断更全局的结构。这些跨越许多时间步的“慢特征”通常更有趣（例如语音中的音素和语调、图像中的对象或书中的故事情节）</p>
<h5 id="模型结构">模型结构</h5>
<p><img src="https://raw.githubusercontent.com/ultramarine-indigo/ultramarine-indigo.github.io/main/imgs/CPC framework.png" alt="CPC framework" style="zoom:67%;" /></p>
<ul>
<li>语言降采样：对于一段语音信号，采样率16000Hz。通过5层卷积CNN，每层的步长为5，4，2，2，2，通过这五层，相当于信号被采样了5×4×2×2×2=160。所以最后输出的信号为每秒16000/160=100点。</li>
<li>特征学习：把这100点的输出再喂给循环神经网络GRU，得到context vector。
<ul>
<li>Positive的pair就是未来的一些输出向量vector，就是同一个句子里面，用当前的向量去预测未来。</li>
<li>Negative的pair有不同的设计方式，最简单的设计就是用另外一段语音或另一个人的语音的vector，就是说不同的Speaker的语音的向量越远越好。这就是说话人分类。</li>
</ul></li>
</ul>
<h3 id="wav2vec-2.0">2. wav2vec 2.0</h3>
<p>论文：wav2vec 2.0: A Framework for Self-Supervised Learning of Speech
Representations <a
target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2020/hash/92d1e1eb1cd6f9fba3227870bb6d7f07-Abstract.html">paper</a></p>
<h5 id="基本思想-1">基本思想</h5>
<p>原始音频数据通过多层卷积神经网络编码得到隐变量，隐变量一方面通过Gumbel
softmax量化模块映射为量化隐变量
，另一方面被随机mask一些位置之后输入给Transformers网络得到上下文特征向量。模型在连续语音表示上构建上下文表示，自注意力端到端捕获整个潜在表示序列的依赖关系。</p>
<p>wav2vec
2.0使用类似于InfoNCE的<strong>对比学习</strong>方法在被mask的位置上计算loss来进行自监督预训练。对于模型每个被mask的位置生成的上下文特征向量(context
representation) ，正例是量化模块在当前的位置生成的量化的隐变量(quantized
representation)
，负例是量化模块在当前句子中的被mask之外的其他位置生成的量化的隐变量(quantized
representation)。</p>
<p>在对未标记语音进行预训练后，对模型在有标签的数据上利用Connectionist
Temporal Classification (CTC)
loss微调，最终可用到下游的语言识别任务。</p>
<h5 id="模型结构-1">模型结构</h5>
<figure>
<img
src="https://raw.githubusercontent.com/ultramarine-indigo/ultramarine-indigo.github.io/main/imgs/wav2vec2.0-framework.png"
alt="wav2vec2.0-framework" />
<figcaption aria-hidden="true">wav2vec2.0-framework</figcaption>
</figure>
<p>模型由一个多层卷积特征编码器组成，它以原始音频作为输入并输出潜在语音表示。然后将它们送到
Transformer
以构建上下文的表示。量化模块将特征编码器的输出离散化，以表示自监督目标中的目标。</p>
<ul>
<li>feature
encoder：编码器由若干个block组成，其中每个block包含时间卷积，然后是层归一化和
GELU
激活函数。编码器的原始波形输入归一化为零均值和单位方差。编码器的总步幅决定了输入到
Transformer 的时间步长 T 的数量</li>
<li>上下文表示Transformer：特征编码器的输出被送入Transformer
架构的上下文网络。没有对绝对位置信息进行编码的固定位置嵌入，而是使用卷积层充当相对位置嵌入。卷积的输出后跟一个
GELU 添加到输入中，然后应用层归一化。</li>
<li>Quantization
module：对于自我监督训练，通过乘积量化将特征编码器的输出离散为一组有限的语音表示。乘积量化相当于从多个码本中选择量化表示并将它们连接起来。每个码本利用Gumbel
softmax
以完全可微的方式选择离散码本条目，concat多个码本得到的向量，通过线性变换得到最终离散化表示。</li>
</ul>
<h3 id="predictive-models-预测模型">Predictive models 预测模型</h3>
<p>有一张狗图，喂给一个encoder，得到一个向量，希望做一些预测任务。如果是监督学习，那目标标签target
lable就是“狗”，现在我们是SSL模型，没有标签，那就做伪标签Pseudo
labeling。</p>
<p>先让狗图片经过一个聚类器clustering，假设的到的是class
1(不一定是狗)，假设聚类器性能很强，那大概率输出的就是狗的分类。通过不断迭代，encoder利用clustering得到更好的label来学习特征，clustering利用encoder得到更好的特征来进行聚类得到label。这个架构就像teacher-student模型，二者互相进化，互相变得更好。</p>
<h4 id="hubert">1. HuBERT</h4>
<p>论文：HuBERT: Self-Supervised Speech Representation Learning by
Masked Prediction of Hidden Units <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.07447">paper</a></p>
<h5 id="基本思想-2">基本思想</h5>
<p>自监督的语音表示学习有三个难点：（1）语音中存在多个unit；（2）训练的时候和NLP不同，没有离散的单词或字符输入；（3）每个unit都有不同的长度，且没有相应的标注。</p>
<p>HuBERT利用离线聚类步骤为类似 BERT
的预测损失提供对齐的目标标签，然后再通过类似BERT的mask式loss让模型在连续的语音数据中学习到数据中的声学和语言模型。一个关键因素是仅在掩码区域上应用预测损失，这迫使模型在连续输入上学习组合的声学和语言模型。HuBERT
主要依赖于无监督聚类步骤的一致性，而不是分配的集群标签的内在质量。</p>
<p>具体而言，HuBERT模型输入masked的连续语音特征，以预测预定的集群分配。预测损失仅应用于masked的区域，迫使模型学习未masked的输入的高水平表示，以正确地推断masked的目标。从直觉上讲，HuBERT模型被迫从连续的输入中学习声学和语言模型。首先，该模型需要将未masked的输入建模为有意义的连续潜在表示，该代表映射到经典的声学建模问题。其次，为了减少预测错误，该模型需要捕获学习表示之间的远程时间关系。激发这项工作的一个关键见解是目标的一致性，而不仅仅是其正确性，这使模型能够专注于建模输入数据的顺序结构。</p>
<h5 id="模型结构-2">模型结构</h5>
<figure>
<img
src="https://raw.githubusercontent.com/ultramarine-indigo/ultramarine-indigo.github.io/main/imgs/HuBERT-framework.png"
alt="HuBERT-framework" />
<figcaption aria-hidden="true">HuBERT-framework</figcaption>
</figure>
<p>首先利用unit discovery
system得到label，然后通过HuBERT从mask之后的输入中得到表示，然后计算label和预测之间的交叉熵loss。</p>
<ul>
<li>HuBERT：卷积波形编码器以20ms的帧率生成特征序列。然后，随机掩盖编码了的音频特征，即以p%的时间步被随机选择作为开始序号，从各个序号后面数l
l<em>l</em>个steps被mask。最后将其输入BERT编码器中得到HuBERT的特征序列。</li>
<li>unit discovery
system：利用聚类的方法得到音频的类别标签。使用多个不同参数的k-means模型来进行学习的方法（cluster
ensembles），从而提供综合信息以促进表示学习。聚类过程多次迭代，一开始直接在MFCCs这样的acoustic
features上直接得到label，在学习进行到一定程度时在模型学习到的潜在表示上重新学习聚类模型来替代之前的聚类模型。</li>
</ul>
<h4 id="wavlm">2. WavLM</h4>
<p>论文：WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack
Speech Processing <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2110.13900">paper</a></p>
<h5 id="基本思想-3">基本思想</h5>
<p>语音信号包含多方面的信息，包括说话者身份、副语言、口语内容等，因此为所有语音任务学习通用表示具有挑战性。为了解决这个问题，提出了一种新的预训练模型
WavLM
来解决全栈下游语音任务。WavLM在训练前联合学习掩蔽语音预测和去噪。通过这种方式，WavLM
不仅通过掩码语言建模保持语音内容建模能力，而且还通过语音去噪提高了非 ASR
任务的潜力。可以理解为，WavLM和HuBERT架构一样，数据一样，loss函数一样，但只引入了denosing，它的效果就比HuBERT要好。</p>
<h5 id="模型结构-3">模型结构</h5>
<figure>
<img
src="https://raw.githubusercontent.com/ultramarine-indigo/ultramarine-indigo.github.io/main/imgs/WavLM-framework.png"
alt="WavLM-framework" />
<figcaption aria-hidden="true">WavLM-framework</figcaption>
</figure>
<ul>
<li>WavLM模型：包含一个卷积特征编码器和一个 Transformer
编码器。卷积编码器由七个时间卷积块组成，然后是层归一化和GELU激活层。卷积输出表示被屏蔽作为
Transformer 输入，Transformer
配备了基于卷积的相对位置嵌入层。同时，采用了门控相对位置偏差，它基于
Transformer 自注意力机制中的“键”和“查询”之间的偏移量进行编码。</li>
<li>掩蔽语音去噪和预测框架：提高复杂声学环境的模型的鲁棒性和说话人身份的保存。具体来说，手动模拟噪声/重叠语音作为输入，并预测掩码区域原始语音的伪标签。使用多个说话者和各种背景噪声来模拟嘈杂的语音以进行自我监督的预训练。从每个训练批次中随机选择一些话语，并将它们与随机选择的噪声音频或随机区域的次要话语混合。噪声音频和次要话语是从同一批次、随机裁剪和缩放的随机源能量比中随机选择的。确保重叠区域小于
50%，并将第一个话语中的说话者作为主要说话者。通过掩蔽语音去噪和预测任务，训练模型从噪声/重叠语音中识别主说话人，并用掩码预测损失预测主说话人对应的内容信息。</li>
</ul>
<h3 id="speech-ssl-model-小结">speech SSL model 小结</h3>
<figure>
<img
src="https://raw.githubusercontent.com/ultramarine-indigo/ultramarine-indigo.github.io/main/imgs/speech%20SSL%20model.png"
alt="speech SSL model" />
<figcaption aria-hidden="true">speech SSL model</figcaption>
</figure>
<h5 id="contrastive-models-对比学习模型-1">Contrastive Models
对比学习模型</h5>
<ul>
<li>CPC：利用对比学习思想，通过当前向量来预测之后的特征。</li>
<li>wav2vec
2.0：通过引入量化模块和mask机制，利用对比学习方法在被mask的位置上计算loss来进行自监督预训练。</li>
<li>XLS-R：框架基本和Wav2vec
2.0一样。只是XLS-R是在超大的数据上训练的。包含400000小时，107种语言。</li>
</ul>
<h5 id="predictive-models-预测模型-1">Predictive models 预测模型</h5>
<ul>
<li>HuBERT：语音，提取MFCC特征，经过K-means得到码本作为伪标签。利用BERT的方式，即mask语音的一部分向量，最小化mask掉的那个的输出和聚类得到的码本标签之间的距离，从而训练模型。然后将得到Hubert反过来当成teacher
model，用来继续训练这边的聚类模型，相互迭代。</li>
<li>WavLM：在HuBERT基础上，加入denosing。即将输入改变为混合的语音作为噪音Noisy
speech（比如是两个人讲话的语音），但做伪标签的语音还是用的干净语音。</li>
<li>BEST-RQ：和WavLM相似，只不过其teacher聚类模型更简单，只进行随机初始化，在预训练过程中，它的Projection层和码本都不做更新。还有值得注意一点，它的输入是梅尔谱图。</li>
</ul>
<h2 id="audio-text-multimodal-model-多模态模型">audio-text multimodal
model 多模态模型</h2>
<h3 id="clap">1. CLAP</h3>
<p>论文：CLAP : LEARNING AUDIO CONCEPTS FROM NATURAL LANGUAGE
SUPERVISION <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2206.04769">paper</a></p>
<h5 id="基本思想-4">基本思想</h5>
<p>通过在大量音频-文本对（其中原始配对的为正样本，不配对为负样本）数据集上进行对比训练，从而利用文本作为音频的监督标签，来帮助模型获得更好的音频识别能力。同时，在分类任务时，来计算音频和类标签文本的相似度来分类。消除了使用类标签进行训练的需要，在推理时实现了灵活的类预测，并可以推广到多个下游任务。</p>
<h5 id="模型结构-4">模型结构</h5>
<figure>
<img
src="https://raw.githubusercontent.com/ultramarine-indigo/ultramarine-indigo.github.io/main/imgs/CLAP-framework.png"
alt="CLAP-framework" />
<figcaption aria-hidden="true">CLAP-framework</figcaption>
</figure>
<ul>
<li>音频和文本分别通过audio encoder和text
encoder，得到语音特征和文本特征，之后通过线性映射将语音特征和文本特征投影到一致的多模态空间（即特征维度相同）。语音特征和文本特征点乘计算相似度矩阵，并在相似度矩阵上使用这种对称交叉熵损失来联合训练音频编码器和文本编码器及其线性映射。</li>
<li>对于分类任务，首先，我们使用预训练的编码器和投影层为音频和待分类标签计算音频嵌入和文本嵌入。其次，计算每个测试音频和所有类标签之间的余弦相似度。每个音频将具有与类标签相同的
logits。最后，通过对二元或多类应用 softmax 和 sigmoid 进行多标签分类，将
logits 转换为概率分布。</li>
</ul>
<h3 id="flap">2. FLAP</h3>
<p>论文：FLAP: FAST LANGUAGE-AUDIO PRE-TRAINING <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2311.01615">paper</a></p>
<h5 id="基本思想-5">基本思想</h5>
<p>通过掩蔽、对比学习和重建有效地学习对齐的音频和视觉表示。为了提高效率，FLAP
随机丢弃音频频谱图标记，仅关注剩余的用于自我监督的标记。通过多模态对比学习，FLAP
学习在共享潜在空间中对齐成对的音频和文本表示。值得注意的是，FLAP
通过掩码来利用多个增强视图进行模态间对比，并学习重建音频标记的掩码部分。此外，FLAP
利用大型语言模型 (LLM)
来增强文本输入，从而提高性能。这些方法导致更健壮和信息丰富的音频文本表示。</p>
<h5 id="模型结构-5">模型结构</h5>
<figure>
<img
src="https://raw.githubusercontent.com/ultramarine-indigo/ultramarine-indigo.github.io/main/imgs/FLAP-framework.png"
alt="FLAP-framework" />
<figcaption aria-hidden="true">FLAP-framework</figcaption>
</figure>
<ul>
<li>音频的 mel 频谱转为patch embedding，之后进行mask，包括1-D
masking和2-D masking两种方式。masked频谱通过encoder（即MAViL）得到audio
embedding。
<ul>
<li>1-D masking：先加入position
embedding，之后在时间维度随机采样进行mask。</li>
<li>2-D masking：现在时间维度将patch
embedding划分为M组，每个组中有K个连续帧。组之间和组内都进行随机采样，之后将采样后的拼接在一起，就得到了masked频谱。本质上将整个序列
(N) 拆分为许多 (M ) 细粒度段
(K)，因此可以通过同质采样并在每个细粒度段中删除来实现更结构化的采样。</li>
</ul></li>
<li>音频-文本对比学习：text通过text encoder得到text
embedding，其中，text是利用AED+LLM来进行了文本增强。之后text
embedding和audio embedding计算InfoNCE loss来进行对比学习</li>
<li>重建音频：audio embedding通过audio
decoder来进行重建频谱，和原始未masked的频谱计算MSE loss。</li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/" rel="tag"># 多模态</a>
              <a href="/tags/speech-model/" rel="tag"># speech model</a>
              <a href="/tags/Self-Supervised-Learning/" rel="tag"># Self-Supervised Learning</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/08/06/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86%E6%80%BB%E7%BB%93/" rel="prev" title="多模态论文梳理总结">
                  <i class="fa fa-angle-left"></i> 多模态论文梳理总结
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2023</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Ultramarine</span>
  </div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script size="300" alpha="0.6" zIndex="-1" src="https://cdnjs.cloudflare.com/ajax/libs/ribbon.js/1.0.2/ribbon.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">false</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
