<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.2.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"ultramarine-indigo.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.18.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="在计算机视觉领域，图像分割是一项关键任务，用于将图像中的目标对象从背景中准确提取出来。为了实现这一目标，研究人员开发了多种不同的图像分割方法。本文将介绍三种常见的图像分割方法：GraphCut、GrabCut和Deep GrabCut。">
<meta property="og:type" content="article">
<meta property="og:title" content="图像分割：GraphCut-GrabCut-DeepGrabCut详解">
<meta property="og:url" content="http://ultramarine-indigo.github.io/2023/06/10/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%EF%BC%9AGraphCut-GrabCut-Deep-GrabCut%E8%AF%A6%E8%A7%A3/index.html">
<meta property="og:site_name" content="ultramarine&#39;s Blog">
<meta property="og:description" content="在计算机视觉领域，图像分割是一项关键任务，用于将图像中的目标对象从背景中准确提取出来。为了实现这一目标，研究人员开发了多种不同的图像分割方法。本文将介绍三种常见的图像分割方法：GraphCut、GrabCut和Deep GrabCut。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://tvax2.sinaimg.cn/large/007RyU1Zly1himjibeg3qj30me0t2h18.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/ultramarine-indigo/ultramarine-indigo.github.io/main/imgs/007RyU1Zly1himjl8tjjcj310408ajws.jpg">
<meta property="article:published_time" content="2023-06-10T09:11:59.000Z">
<meta property="article:modified_time" content="2023-10-07T03:47:13.472Z">
<meta property="article:author" content="Ultramarine">
<meta property="article:tag" content="CV">
<meta property="article:tag" content="图像分割">
<meta property="article:tag" content="算法模型">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://tvax2.sinaimg.cn/large/007RyU1Zly1himjibeg3qj30me0t2h18.jpg">


<link rel="canonical" href="http://ultramarine-indigo.github.io/2023/06/10/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%EF%BC%9AGraphCut-GrabCut-Deep-GrabCut%E8%AF%A6%E8%A7%A3/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://ultramarine-indigo.github.io/2023/06/10/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%EF%BC%9AGraphCut-GrabCut-Deep-GrabCut%E8%AF%A6%E8%A7%A3/","path":"2023/06/10/图像分割：GraphCut-GrabCut-Deep-GrabCut详解/","title":"图像分割：GraphCut-GrabCut-DeepGrabCut详解"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>图像分割：GraphCut-GrabCut-DeepGrabCut详解 | ultramarine's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">ultramarine's Blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section">首页</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section">关于</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section">标签</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section">分类</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section">归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-GraphCut"><span class="nav-number">1.</span> <span class="nav-text">1.GraphCut</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-%E4%B8%BB%E8%A6%81%E6%80%9D%E6%83%B3"><span class="nav-number">1.1.</span> <span class="nav-text">1.1 主要思想</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-%E6%A8%A1%E5%9E%8B%E8%AE%BE%E8%AE%A1"><span class="nav-number">1.2.</span> <span class="nav-text">1.2 模型设计</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-GrabCut"><span class="nav-number"></span> <span class="nav-text">2.GrabCut</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-%E4%B8%BB%E8%A6%81%E6%80%9D%E6%83%B3"><span class="nav-number">0.1.</span> <span class="nav-text">2.1 主要思想</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-%E6%A8%A1%E5%9E%8B%E8%AE%BE%E8%AE%A1"><span class="nav-number">0.2.</span> <span class="nav-text">2.2 模型设计</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B"><span class="nav-number">0.3.</span> <span class="nav-text">2.3 算法流程</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Deep-GrabCut"><span class="nav-number"></span> <span class="nav-text">3.Deep GrabCut</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-%E4%B8%BB%E8%A6%81%E6%80%9D%E6%83%B3"><span class="nav-number">0.1.</span> <span class="nav-text">3.1 主要思想</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-%E7%9F%A9%E5%BD%A2%E9%87%87%E6%A0%B7"><span class="nav-number">0.1.1.</span> <span class="nav-text">1. 矩形采样</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-%E7%9F%A9%E5%BD%A2%E8%BD%AC%E6%8D%A2"><span class="nav-number">0.1.2.</span> <span class="nav-text">2.矩形转换</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-CEDN-model-training"><span class="nav-number">0.1.3.</span> <span class="nav-text">3. CEDN model training</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Ultramarine</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives">
          <span class="site-state-item-count">3</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://ultramarine-indigo.github.io/2023/06/10/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%EF%BC%9AGraphCut-GrabCut-Deep-GrabCut%E8%AF%A6%E8%A7%A3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ultramarine">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ultramarine's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="图像分割：GraphCut-GrabCut-DeepGrabCut详解 | ultramarine's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          图像分割：GraphCut-GrabCut-DeepGrabCut详解
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-06-10 17:11:59" itemprop="dateCreated datePublished" datetime="2023-06-10T17:11:59+08:00">2023-06-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AE%BA%E6%96%87%E8%AF%A6%E8%A7%A3/" itemprop="url" rel="index"><span itemprop="name">论文详解</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>在计算机视觉领域，图像分割是一项关键任务，用于将图像中的目标对象从背景中准确提取出来。为了实现这一目标，研究人员开发了多种不同的图像分割方法。本文将介绍三种常见的图像分割方法：GraphCut、GrabCut和Deep GrabCut。</p>
<span id="more"></span>

<p><strong>GraphCut</strong> 是一种经典的图像分割方法，它使用图割算法，基于用户提供的前景和背景标记，将图像分割成两个区域。GraphCut的核心思想是通过最小割最大流算法找到能量最小的切割，将图像分成目标和背景部分。</p>
<p><strong>GrabCut</strong> 是GraphCut的改进版本，引入了RGB三通道的混合高斯模型来更好地建模目标和背景。</p>
<p><strong>Deep GrabCut</strong> 是基于深度学习的图像分割方法。它利用卷积编码器-解码器网络进行训练，同时使用距离图作为输入，以捕捉上下文信息。这种方法通过深度学习技术提高了分割性能。</p>
<h3 id="1-GraphCut"><a href="#1-GraphCut" class="headerlink" title="1.GraphCut"></a>1.GraphCut</h3><p>提出论文：《Interactive graph cuts for optimal boundary and region segmentation of objects in N-D images》 (Boykov，iccv01)</p>
<h4 id="1-1-主要思想"><a href="#1-1-主要思想" class="headerlink" title="1.1 主要思想"></a>1.1 主要思想</h4><ol>
<li>首先，用户明确指出少量背景像素B和前景目标像素O。然后将图像建立为图（Graph），像素就是图上的节点(Node)。</li>
<li>定义两个特殊的节点是T节点（background terminal, 表示背景）以及S节点(Object Terminal, 表示前景）。在所有节点之间建立连接。图像像素之间的连接称作n-links，像素到特殊节点之间的连接称作t-links</li>
<li>为所有的连接赋以能量或权重。t-links的能量代表着一个像素有多么像前景或背景，所有下图中那些已经被标注为前景或背景的像素对应的t-links很粗。n-links的能量约束着像素之间是否会被分配为不同的区域，下图中如果相邻节点越有可能被分开，其连接越细，能量越小。</li>
<li>一个cut（割）就是图中边集合E的一个子集C，那这个割的cost（表示为|C|）就是边子集C的所有边的权值的总和。Graph Cuts中的Cuts是指这样一个边的集合，很显然这些边集合包括了上面2种边，该集合中所有边的断开会导致残留”S”和”T”图的分开，所以就称为“割”。max-flow&#x2F;min-cut算法就可以用来获得s-t图的最小割，即其边的所有权值之和最小。这个最小割把图的顶点划分为两个不相交的子集S和T，其中s ∈S，t∈ T和S∪T&#x3D;V 。这两个子集就对应于图像的前景像素集和背景像素集，那就相当于完成了图像分割。</li>
</ol>
<h4 id="1-2-模型设计"><a href="#1-2-模型设计" class="headerlink" title="1.2 模型设计"></a>1.2 模型设计</h4><p>假设整幅图像可以表示为一系列灰度值（每个像素的灰度值）<strong>I</strong>&#x3D;{I1,I2,….Ip}，而整个图像的标签label（每个像素的label）为<strong>A</strong>&#x3D; {A1,A2,…,Ap }，其中Ai为0（背景）或者1（目标）。那假设图像的分割为A时，图像的能量可以表示为：</p>
<p>$$<br>E(A)&#x3D;\lambda \cdot R(A)+B(A)<br>$$<br>where<br>$$<br>\begin{aligned}<br>&amp; R(A)&#x3D;\sum_{p \in \mathcal{P} } R_p\left(A_p\right) \<br>&amp; B(A)&#x3D;\sum_{ {p, q} \in \mathcal{N} } B_{ {p, q} }: \delta\left(A_p, A_q\right)<br>\end{aligned}<br>$$<br>and<br>$$<br>\delta\left(A_p, A_q\right)&#x3D;\left{\begin{array}{cc}<br>1 &amp; \text { if } A_p \neq A_q \<br>0 &amp; \text { otherwise }<br>\end{array}\right.<br>$$<br>其中，R(A)为区域项（regional term），B(A)为边界项（boundary term），而a就是区域项和边界项之间的重要因子，决定它们对能量的影响大小。如果a为0，那么就只考虑边界因素，不考虑区域因素。E(A)表示的是权值，即损失函数，也叫能量函数，图割的目标就是优化能量函数使其值达到最小。</p>
<p><strong>区域项：</strong></p>
<p>其中表示为像素Ip分配标签Ap的惩罚，Rp(Ap)能量项的权值可以通过比较像素Ip的灰度和给定的目标和前景的灰度直方图来获得，换句话说就是像素Ip属于标签Ap的概率，我们希望像素Ip分配为其概率最大的标签Ap，这时候我们希望能量最小，所以一般取概率的负对数值，故t-link的权值如下：</p>
<p>$$<br>\begin{aligned}<br>R_p \text { (“obj”) } &amp; &#x3D;-\ln \operatorname{Pr}\left(I_p \mid \mathcal{O}\right) \<br>R_p \text { (“bkg”) } &amp; &#x3D;-\ln \operatorname{Pr}\left(I_p \mid \mathcal{B}\right) .<br>\end{aligned}<br>$$<br>​    由上面两个公式可以看到，当像素Ip的灰度值属于目标的概率Pr(Ip|’obj’)大于背景Pr(Ip|’bkg’)，那么Rp(1)就小于Rp(0)，也就是说当像素Ip更有可能属于目标时，将Ip归类为目标就会使能量R(A)小。那么，如果全部的像素都被正确划分为目标或者背景，那么这时候能量就是最小的</p>
<p><strong>边界项：</strong></p>
<p>$$<br>\begin{gathered}<br>B_{ {p, q} } \propto \exp \left(-\frac{\left(I_p-I_q\right)^2}{2 \sigma^2}\right) \cdot \frac{1}{\operatorname{dist}(p, q)} . \<br>\end{gathered}<br>$$<br>其中，p和q为邻域像素，边界平滑项主要体现分割L的边界属性，B可以解析为像素p和q之间不连续的惩罚，一般来说如果p和q越相似（例如它们的灰度），那么B越大，如果他们非常不同，那么B就接近于0。换句话说，如果两邻域像素差别很小，那么它属于同一个目标或者同一背景的可能性就很大，如果他们的差别很大，那说明这两个像素很有可能处于目标和背景的边缘部分，则被分割开的可能性比较大，所以当两邻域像素差别越大，B越小，即能量越小。</p>
<h2 id="2-GrabCut"><a href="#2-GrabCut" class="headerlink" title="2.GrabCut"></a>2.GrabCut</h2><p>提出论文：《“GrabCut” — Interactive Foreground Extraction using Iterated Graph Cuts》</p>
<h4 id="2-1-主要思想"><a href="#2-1-主要思想" class="headerlink" title="2.1 主要思想"></a>2.1 主要思想</h4><ol>
<li>Graph Cut的目标和背景的模型是灰度直方图，Grab Cut取代为RGB三通道的混合高斯模型GMM；</li>
<li>Graph Cut的能量最小化（分割）是一次达到的，而Grab Cut取代为一个不断进行分割估计和模型参数学习的交互迭代过程；</li>
<li>Graph Cut需要用户指定目标和背景的一些种子点，但是Grab Cut只需要提供背景区域的像素集就可以了。也就是说你只需要框选目标，那么在方框外的像素全部当成背景，这时候就可以对GMM进行建模和完成良好的分割了。即Grab Cut允许不完全的标注（incomplete labelling）。</li>
</ol>
<h4 id="2-2-模型设计"><a href="#2-2-模型设计" class="headerlink" title="2.2 模型设计"></a>2.2 模型设计</h4><p>我们采用RGB颜色空间，分别用一个K个高斯分量（一取般K&#x3D;5）的全协方差GMM（混合高斯模型）来对目标和背景进行建模。于是就存在一个额外的向量$k&#x3D;{k_1,…k_n}$其中$k_n$就是第n个像素对应于哪个高斯分量，。$ k_n \in\left{1, \ldots, K\right}$对于每个像素，其来自于目标GMM的某个高斯分量，或者背景GMM的某个高斯分量。</p>
<p>所以用于分割整个图像的Gibbs能量为</p>
<p>$$<br>\mathbf{E}(\underline{\alpha}, \mathbf{k}, \underline{\theta}, \mathbf{z})&#x3D;U(\underline{\alpha}, \mathbf{k}, \underline{\theta}, \mathbf{z})+V(\underline{\alpha}, \mathbf{z}),<br>$$<br>根据GMM（高斯混合模型）中变量 $\mathbf{k}$ 的不同情况，数据项 $U$ 现在被定义为考虑到颜色GMM模型的情况。<br>$$<br>U(\underline{\alpha}, \mathbf{k}, \underline{\theta}, \mathbf{z})&#x3D;\sum_n D\left(\alpha_n, k_n, \underline{\theta}, z_n\right),<br>$$<br>其中 $D\left(\alpha_n, k_n, \underline{\theta}, z_n\right)&#x3D;-\log p\left(z_n \mid \alpha_n, k_n, \underline{\theta}\right)-\log \pi\left(\alpha_n, k_n\right)$，$p(\cdot)$ 是高斯概率分布，$\pi(\cdot)$ 是混合权重系数，因此（除去一个常数）：<br>$$<br>\begin{array}{r}<br>D\left(\alpha_n, k_n, \underline{\theta}, z_n\right)&#x3D;-\log \pi\left(\alpha_n, k_n\right)+\frac{1}{2} \log \operatorname{det} \Sigma\left(\alpha_n, k_n\right) \<br>+\frac{1}{2}\left[z_n-\mu\left(\alpha_n, k_n\right)\right]^{\top} \Sigma\left(\alpha_n, k_n\right)^{-1}\left[z_n-\mu\left(\alpha_n, k_n\right)\right] .<br>\end{array}<br>$$<br>因此，现在模型的参数是：<br>$$<br>\underline{\theta}&#x3D;{\pi(\alpha, k), \mu(\alpha, k), \Sigma(\alpha, k), \alpha&#x3D;0,1, k&#x3D;1 \ldots K},<br>$$<br><strong>区域项</strong></p>
<p>表示一个像素被归类为目标或者背景的惩罚，也就是某个像素属于目标或者背景的概率的负对数。我们知道混合高斯密度模型是如下形式</p>
<p>$$<br>V(\underline{\alpha}, \mathbf{z})&#x3D;\gamma \sum_{(m, n) \in \mathbf{C}}\left[\alpha_n \neq \alpha_m\right] \exp -\beta\left|z_m-z_n\right|^2 .<br>$$<br>取负对数之后就变成式（9）那样的形式了，其中GMM的参数<strong>θ</strong>就有三个：每一个高斯分量的权重π、每个高斯分量的均值向量<strong>u</strong>（因为有RGB三个通道，故为三个元素向量）和协方差矩阵<strong>∑</strong>（因为有RGB三个通道，故为3x3矩阵）。如式（10）。也就是说描述目标的GMM和描述背景的GMM的这三个参数都需要学习确定。一旦确定了这三个参数，那么我们知道一个像素的RGB颜色值之后，就可以代入目标的GMM和背景的GMM，就可以得到该像素分别属于目标和背景的概率了，也就是Gibbs能量的区域能量项。</p>
<p><strong>区域项：</strong><br>$$<br>\begin{aligned}<br>&amp; D(x)&#x3D;\sum_{i&#x3D;1}^K \pi_i g_i\left(x ; \mu_i, \sum_i\right) \quad, \sum_{i&#x3D;1}^K \pi_i&#x3D;1 \text { 且 } 0 \leq \pi_i \leq 1 \<br>&amp; g(x ; \mu, \Sigma)&#x3D;\frac{1}{\sqrt{(2 \pi)^d|\Sigma|} } \exp \left[-\frac{1}{2}(x-\mu)^T \Sigma^{-1}(x-\mu)\right]<br>\end{aligned}<br>$$<br>体现邻域像素m和n之间不连续的惩罚，如果两邻域像素差别很小，那么它属于同一个目标或者同一背景的可能性就很大，如果他们的差别很大，那说明这两个像素很有可能处于目标和背景的边缘部分，则被分割开的可能性比较大，所以当两邻域像素差别越大，能量越小。而在RGB空间中，衡量两像素的相似性，我们采用欧式距离（二范数）。这里面的参数β由图像的对比度决定，可以想象，如果图像的对比度较低，也就是说本身有差别的像素m和n，它们的差||zm-zn||还是比较低，那么我们需要乘以一个比较大的β来放大这种差别，而对于对比度高的图像，那么也许本身属于同一目标的像素m和n的差||zm-zn||还是比较高，那么我们就需要乘以一个比较小的β来缩小这种差别，使得V项能在对比度高或者低的情况下都可以正常工作。常数γ一般可设为50（经过作者用15张图像训练得到的比较好的值）。</p>
<h4 id="2-3-算法流程"><a href="#2-3-算法流程" class="headerlink" title="2.3 算法流程"></a>2.3 算法流程</h4><p><img src="https://tvax2.sinaimg.cn/large/007RyU1Zly1himjibeg3qj30me0t2h18.jpg" alt="Grabcut algorithm-flow"></p>
<p><strong>1.初始化</strong></p>
<ul>
<li>用户通过直接框选目标来得到一个初始的trimap T，即方框外的像素全部作为背景像素TB，而方框内TU的像素全部作为“可能是目标”的像素。</li>
<li>对TB内的每一像素n，初始化像素n的标签αn&#x3D;0，即为背景像素；而对TU内的每个像素n，初始化像素n的标签αn&#x3D;1，即作为“可能是目标”的像素。</li>
<li>通过初始划分的像素来估计目标和背景的GMM了。我们可以通过k-means算法分别把属于目标和背景的像素聚类为K类，即GMM中的K个高斯模型，这时候GMM中每个高斯模型就具有了一些像素样本集，这时候它的参数均值和协方差就可以通过他们的RGB值估计得到，而该高斯分量的权值可以通过属于该高斯分量的像素个数与总的像素个数的比值来确定。</li>
</ul>
<p><strong>2.迭代最小化</strong></p>
<ol>
<li>对每个像素分配GMM中的高斯分量（例如像素n是目标像素，那么把像素n的RGB值代入目标GMM中的每一个高斯分量中，概率最大的那个就是最有可能生成n的，也即像素n的第kn个高斯分量）</li>
<li>对于给定的图像数据z，学习优化GMM的参数（因为在步骤1中我们已经为每个像素归为哪个高斯分量做了归类，那么每个高斯模型就具有了一些像素样本集，这时候它的参数均值和协方差就可以通过这些像素样本的RGB值估计得到，而该高斯分量的权值可以通过属于该高斯分量的像素个数与总的像素个数的比值来确定。）</li>
<li>分割估计（建立图，分析的Gibbs能量项，并求出权值t-link和n-link，然后通过max flow&#x2F;min cut算法来进行分割）</li>
<li>重复步骤1到3，直到收敛。经过3的分割后，每个像素属于目标GMM还是背景GMM就变了，所以每个像素的kn就变了，故GMM也变了，所以每次的迭代会交互地优化GMM模型和分割结果。另外，因为步骤1到3的过程都是能量递减的过程，所以可以保证迭代过程会收敛。</li>
<li>采用border matting对分割的边界进行平滑等等后期处理。</li>
</ol>
<p><strong>3. 用户编辑（交互）</strong></p>
<ul>
<li>编辑：人为地固定一些像素是目标或者背景像素，然后再执行一次2中步骤3；</li>
<li>重操作：重复整个迭代算法。</li>
</ul>
<h2 id="3-Deep-GrabCut"><a href="#3-Deep-GrabCut" class="headerlink" title="3.Deep GrabCut"></a>3.Deep GrabCut</h2><h4 id="3-1-主要思想"><a href="#3-1-主要思想" class="headerlink" title="3.1 主要思想"></a>3.1 主要思想</h4><p><img src="https://raw.githubusercontent.com/ultramarine-indigo/ultramarine-indigo.github.io/main/imgs/007RyU1Zly1himjl8tjjcj310408ajws.jpg" alt="DeepGrabCut-framework"></p>
<ol>
<li>将 bbox 转换为欧式距离图（大小与输入的图像大小一致），将其作为一种软约束，在输入训练网络时，只需要将该欧式距离图和原 RGB 图像连接起来（即在通道数上进行扩展）</li>
<li>构建卷积编码器-解码器的网络(convolutional encoder-decoder network) 作为模型框架，通过将含有距离图的图像作为输入，进行模型的训练。</li>
</ol>
<h5 id="1-矩形采样"><a href="#1-矩形采样" class="headerlink" title="1. 矩形采样"></a>1. 矩形采样</h5><p>通过采样各个分割目标的矩形框，扩充出一个训练集，同时还能保证不会过拟合。对于每个实例，它的 ground truth 是最紧挨着物体边缘的 bounding box $B^0$ ，表示为 $\left[x_{\text {min }}^0, y_{\text {min }}^0, x_{\text {max }}^0, y_{\text {max }}^0\right]$ ，即 bounding box 的左下角 $\left(\left[x_{\text {min }}^0, y_{\text {min }}^0\right]\right)$ 和右上角 $\left(\left[x_{\text {max }}^0, y_{\text {max }}^0\right]\right)$ 。<br>随机采样 $N_{\text {train }}$ 个矩形框，各个矩形框 $B^i, i \in\left{1, \ldots, N_{\text {train }}\right}$ 的两个点 (四个坐标) 为:<br>$$<br>x_{\min &#x2F; \max }^i&#x3D;x_{\min &#x2F; \max }^0+v \cdot g_j^i \cdot\left(x_{\max }^0-x_{\min }^0\right)<br>$$</p>
<p>$$<br>y_{\min &#x2F; \max }^i&#x3D;y_{\min &#x2F; \max }^0+v \cdot g_j^i \cdot\left(y_{\max }^0-y_{\min }^0\right)<br>$$</p>
<p>其中， $v$ 是一个超参数，控制矩形框的偏移程度， $g_j^i$ 服从标准正态分布 $N(0,1) ， j \in{1,2,3,4}$ 为 standard Gaussian random variables</p>
<h5 id="2-矩形转换"><a href="#2-矩形转换" class="headerlink" title="2.矩形转换"></a>2.矩形转换</h5><p>给定图像 L 中的一个目标物体矩形框 B，定义矩形框 B 上的 pixel 为一个像素集合：Se &#x3D; { pi∣pi is on the  edge of B } ，其中 pi 表示了像素 i 的位置。</p>
<p>同样地，<br>定义矩形框 B 内(inside)的 pixel 为一个像素集合：Si</p>
<p>定义矩形框 B 外(outside)的 pixel 为一个像素集合：So</p>
<p>创建 2D 图像的 distance map D，该 distance map 与原图像有着相同的宽度和高度</p>
<p>distance map D中，位置 pi 的像素距离计算：</p>
<p>$$<br>\mathbf{D}\left(\mathbf{p}_i\right)&#x3D; \begin{cases}128-\min _{\forall \mathbf{p}_j \in \mathcal{S}_e}\left|\mathbf{p}_i-\mathbf{p}_j\right|, &amp; \text { if } \mathbf{p}_i \in \mathcal{S}_i, \ 128, &amp; \text { if } \mathbf{p}_i \in \mathcal{S}_e, \ 128+\min _{\forall \mathbf{p}_j \in \mathcal{S}_e}\left|\mathbf{p}_i-\mathbf{p}_j\right|, &amp; \text { if } \mathbf{p}_i \in \mathcal{S}_o\end{cases}<br>$$<br>其中，∣ ⋅ ∣为欧几里得距离，本文使用有符号的距离变换来捕获输入矩形框的上下文信息。考虑到存储效率，将距离 D 截取到 0 ~ 255，最后将 distance map D 和 RGB 图像进行连接，送入训练网络进行训练。</p>
<h5 id="3-CEDN-model-training"><a href="#3-CEDN-model-training" class="headerlink" title="3. CEDN model training"></a>3. CEDN model training</h5><p>输入： RGB 与 distance map D 的 4 通道连接图<br>输出： 二值分割结果</p>
<p>CEDN model（Convolutional Encoder-Decoder Network）包含两个部分：encoder &amp; decoder</p>
<p>Encoder： 由 convolutional layer 和 max-pooling layer 组成，作用是提取深层信息，将输入图片逐渐抽象成更小的 feature map。</p>
<p>Decoder： 由 convolutional layer 和 unpooling layer 组成，作用是根据 feature map 重建图像细节信息，还原到输入图像的大小</p>
<p>论文中，使用 VGG-16 的前 14 层参数来初始化 encoder 网络，而且由于 VGG 输入图像是三通道的，这里是 4 通道，所以第一层 convolutional filters 的第四通道参数初始化为 0。decoder的网络结构比encoder 更加简洁，以减少冗余的参数并加快训练。其中decoder的参数由 Xavier 初始化，为解决过拟合问题，decoder 网络中在每个 conv layer 后都使用了 dropout 层。此外，在每个 training epoch 的开始阶段，随机地对所有训练数据进行重新采样。</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/CV/" rel="tag"># CV</a>
              <a href="/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/" rel="tag"># 图像分割</a>
              <a href="/tags/%E7%AE%97%E6%B3%95%E6%A8%A1%E5%9E%8B/" rel="tag"># 算法模型</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/12/16/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" rel="prev" title="数据库学习笔记">
                  <i class="fa fa-angle-left"></i> 数据库学习笔记
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2023</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Ultramarine</span>
  </div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script size="300" alpha="0.6" zIndex="-1" src="https://cdnjs.cloudflare.com/ajax/libs/ribbon.js/1.0.2/ribbon.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  






  





</body>
</html>
