<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.2.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"ultramarine-indigo.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.18.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="对比学习（Contrastive Learning）是一种机器学习的方法，其目标是通过学习输入数据中不同样本之间的差异，从而提取有用的表示或特征。该方法通常用于无监督学习或自监督学习的场景，其中模型被设计成学习数据中的内在结构，而无需显式的标签。">
<meta property="og:type" content="article">
<meta property="og:title" content="对比学习梳理总结">
<meta property="og:url" content="http://ultramarine-indigo.github.io/2023/07/10/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A2%B3%E7%90%86%E6%80%BB%E7%BB%93/index.html">
<meta property="og:site_name" content="ultramarine&#39;s Blog">
<meta property="og:description" content="对比学习（Contrastive Learning）是一种机器学习的方法，其目标是通过学习输入数据中不同样本之间的差异，从而提取有用的表示或特征。该方法通常用于无监督学习或自监督学习的场景，其中模型被设计成学习数据中的内在结构，而无需显式的标签。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/ultramarine-indigo/ultramarine-indigo.github.io/main/imgs/SimSiam-framework.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ultramarine-indigo/ultramarine-indigo.github.io/main/imgs/Comparison on Siamese architectures.png">
<meta property="article:published_time" content="2023-07-10T03:43:36.000Z">
<meta property="article:modified_time" content="2023-12-13T04:18:37.311Z">
<meta property="article:author" content="Ultramarine">
<meta property="article:tag" content="对比学习">
<meta property="article:tag" content="无监督学习">
<meta property="article:tag" content="NCE loss">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/ultramarine-indigo/ultramarine-indigo.github.io/main/imgs/SimSiam-framework.png">


<link rel="canonical" href="http://ultramarine-indigo.github.io/2023/07/10/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A2%B3%E7%90%86%E6%80%BB%E7%BB%93/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://ultramarine-indigo.github.io/2023/07/10/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A2%B3%E7%90%86%E6%80%BB%E7%BB%93/","path":"2023/07/10/对比学习梳理总结/","title":"对比学习梳理总结"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>对比学习梳理总结 | ultramarine's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">ultramarine's Blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section">首页</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section">关于</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section">标签</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section">分类</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section">归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0"><span class="nav-text">对比学习概述</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6"><span class="nav-text">对比学习框架</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-text">目标函数&#x2F;损失函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E9%87%87%E7%94%A8%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-text">对比学习采用损失函数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A3%E7%90%86%E4%BB%BB%E5%8A%A1"><span class="nav-text">代理任务</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86"><span class="nav-text">对比学习论文梳理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E9%98%B6%E6%AE%B5%E7%99%BE%E8%8A%B1%E9%BD%90%E6%94%BE"><span class="nav-text">第一阶段：百花齐放</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#instdiscinstance-discrimination"><span class="nav-text">InstDisc（instance
discrimination）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3"><span class="nav-text">基本思想</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%B8%BB%E4%BD%93%E6%96%B9%E6%B3%95"><span class="nav-text">主体方法</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C%E7%BB%86%E8%8A%82"><span class="nav-text">实验细节</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#cpc"><span class="nav-text">CPC</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3-1"><span class="nav-text">基本思想</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%B8%BB%E4%BD%93%E6%96%B9%E6%B3%95-1"><span class="nav-text">主体方法</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#cmc"><span class="nav-text">CMC</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3-2"><span class="nav-text">基本思想</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%B8%BB%E4%BD%93%E6%96%B9%E6%B3%95-2"><span class="nav-text">主体方法</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%80%BB%E7%BB%93-1"><span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%BB%E7%BB%93-2"><span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E9%98%B6%E6%AE%B5%E6%A8%A1%E5%9E%8B%E7%BB%86%E8%8A%82%E7%BB%9F%E4%B8%80"><span class="nav-text">第二阶段：模型细节统一</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#moco"><span class="nav-text">MoCo</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3-3"><span class="nav-text">基本思想</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%A0%94%E7%A9%B6%E5%8A%A8%E6%9C%BA"><span class="nav-text">研究动机</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%B8%BB%E4%BD%93%E6%96%B9%E6%B3%95-3"><span class="nav-text">主体方法</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%80%BB%E7%BB%93-3"><span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#simclr"><span class="nav-text">SimCLR</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%B8%BB%E8%A6%81%E6%80%9D%E8%B7%AF"><span class="nav-text">主要思路</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%B8%BB%E4%BD%93%E6%96%B9%E6%B3%95-4"><span class="nav-text">主体方法</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%80%BB%E7%BB%93-4"><span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#swav"><span class="nav-text">SwAV</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%80%9D%E8%B7%AF"><span class="nav-text">基本思路</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%B8%BB%E4%BD%93%E6%96%B9%E6%B3%95-5"><span class="nav-text">主体方法</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E7%BB%86%E8%8A%82multi-crop"><span class="nav-text">训练细节——multi crop</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%BB%E7%BB%93-5"><span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%B8%89%E9%98%B6%E6%AE%B5%E4%B8%8D%E7%94%A8%E8%B4%9F%E6%A0%B7%E6%9C%AC"><span class="nav-text">第三阶段：不用负样本</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#byol"><span class="nav-text">BYOL</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3-4"><span class="nav-text">基本思想</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%B8%BB%E4%BD%93%E6%96%B9%E6%B3%95-6"><span class="nav-text">主体方法</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#simsiam"><span class="nav-text">SimSiam</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3-5"><span class="nav-text">基本思想</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%B8%BB%E4%BD%93%E6%96%B9%E6%B3%95-7"><span class="nav-text">主体方法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93"><span class="nav-text">对比学习模型总结</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E9%98%B6%E6%AE%B5%E7%99%BE%E8%8A%B1%E9%BD%90%E6%94%BE-1"><span class="nav-text">第一阶段：百花齐放</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E9%98%B6%E6%AE%B5%E6%A8%A1%E5%9E%8B%E7%BB%86%E8%8A%82%E7%BB%9F%E4%B8%80-1"><span class="nav-text">第二阶段：模型细节统一</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AC%AC%E4%B8%89%E9%98%B6%E6%AE%B5%E4%B8%8D%E7%94%A8%E8%B4%9F%E6%A0%B7%E6%9C%AC%E7%9A%84%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0"><span class="nav-text">第三阶段：不用负样本的对比学习</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Ultramarine"
      src="/images/header.jpg">
  <p class="site-author-name" itemprop="name">Ultramarine</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives">
          <span class="site-state-item-count">13</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags">
        <span class="site-state-item-count">21</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://ultramarine-indigo.github.io/2023/07/10/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%A2%B3%E7%90%86%E6%80%BB%E7%BB%93/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/header.jpg">
      <meta itemprop="name" content="Ultramarine">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ultramarine's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="对比学习梳理总结 | ultramarine's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          对比学习梳理总结
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-07-10 11:43:36" itemprop="dateCreated datePublished" datetime="2023-07-10T11:43:36+08:00">2023-07-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AE%BA%E6%96%87%E8%AF%A6%E8%A7%A3/" itemprop="url" rel="index"><span itemprop="name">论文详解</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>对比学习（Contrastive
Learning）是一种机器学习的方法，其目标是通过学习输入数据中不同样本之间的差异，从而提取有用的表示或特征。该方法通常用于无监督学习或自监督学习的场景，其中模型被设计成学习数据中的内在结构，而无需显式的标签。</p>
<span id="more"></span>
<h2 id="对比学习概述">对比学习概述</h2>
<p>对比学习的基本思想是通过将样本与其它样本进行比较，使得相似的样本在表示空间中更加接近，而不相似的样本在表示空间中更加分散。这样的学习方式使得模型能够更好地捕捉数据中的结构和模式。</p>
<p>对比学习的两个重要概念：对齐性(alignment)和均匀性(uniformity)。由于对比学习的表示一般都会正则化，因而会集中在一个超球面上。对齐性和均匀性指的是好的表示空间应该满足两个条件：一个是相近样例的表示尽量接近，即对齐性；而不相近样例的表示应该均匀的分布在超球面上，即均匀性。满足这样条件的表示空间是线性可分的，即一个线性分类器就足以用来分类，因而也是我们希望得到的，我们可以通过这两个特性来分析表示空间的好坏。</p>
<h3 id="对比学习框架">对比学习框架</h3>
<p>框架：通过定义正负样本的规则，自动构造相似实例和不相似实例，要求习得一个表示学习模型，通过这个模型，使得相似的实例在投影空间中比较接近，而不相似的实例在投影空间中距离比较远。因此，要构建对比学习模型最关键的是三个问题：正负样例、对比损失以及模型结构。</p>
<p>对比学习具有很好的灵活性，即可以设计不同的正负样本对的规则。</p>
<ul>
<li>视频：同一个视频里的任意两帧是正样本，和其它视频的所有帧是负样本</li>
<li>NLP：simCSE 把同样的句子扔给模型，但是做 2 次 forward，通过不同的
dropout 得到一个句子的 2 个特征；和其它所有句子的特征都是负样本。</li>
<li>CV：一个物体的不同视角 view（正面、背面；RGB
图像、深度图像）作为不同形式的正样本。</li>
<li>多模态领域：Open AI 的 CLIP 模型</li>
</ul>
<h3 id="目标函数损失函数">目标函数/损失函数</h3>
<p>损失/目标函数：衡量模型的预测输出和固定的目标之间的差距(difference)。</p>
<ul>
<li><p>生成式：模型重建样本和样本本身的差距。常见：L1 loss、L2
loss</p></li>
<li><p>判别式：模型预测输出和样本类别标签的差距。常见：cross-entropy
loss、margin-based loss</p></li>
<li><p>对比学习：测量样本对在特征空间的相似性。</p>
<p>相似样本离得近，不相似样本离得远。对比学习与生成式和判别式的区别在于其目标不固定，训练过程中不断改变。目标有编码器抽出来的特征而决定。</p></li>
<li><p>对抗学习：衡量两个概率分布之间的差异</p>
<ul>
<li>对抗学习主要做无监督的数据生成 unsupervised data generation</li>
<li>对抗性的方法做特征学习，如果可以生成很好的数据，模型应该学到数据的底层分布，那么模型也能学习到很好的特征</li>
</ul></li>
</ul>
<h4 id="对比学习采用损失函数">对比学习采用损失函数</h4>
<p><strong>NCE(noise contrastive estimation) loss</strong></p>
<p>NCE（noise contrastive
estimation）核心思想是将多分类问题转化成二分类问题，一个类是数据类别
data sample，另一个类是噪声类别 noisy
sample，通过学习数据样本和噪声样本之间的区别，将数据样本去和噪声样本做对比，也就是“噪声对比（noise
contrastive）”，从而发现数据中的一些特性。但是，如果把整个数据集剩下的数据都当作负样本（即噪声样本），虽然解决了类别多的问题，计算复杂度还是没有降下来，解决办法就是做负样本采样来计算loss，这就是estimation的含义，也就是说它只是估计和近似。一般来说，负样本选取的越多，就越接近整个数据集，效果自然会更好。</p>
<p>字典中特征表示 v 对应于第 i 个示例的概率如下，其中Zi为归一化常数
<span class="math display">\[
P(i \mid \mathbf{v})=\frac{\exp \left(\mathbf{v}^T \mathbf{f}_i /
\tau\right)}{Z_i} \\
Z_i=\sum_{j=1}^n \exp \left(\mathbf{v}_j^T \mathbf{f}_i / \tau\right)
\]</span> 噪声分布形式化为均匀分布：<span
class="math inline">\(P_n=1/n\)</span>。假设噪声样本的频率是数据样本的 m
倍。因此，具有特征 <em>v</em> 的样本 i 属于数据分布的后验概率（表示为
<em>D</em>=1）是 <span class="math display">\[
h(i, \mathbf{v}):=P(D=1 \mid i, \mathbf{v})=\frac{P(i \mid
\mathbf{v})}{P(i \mid \mathbf{v})+m P_n(i)}
\]</span>
我们的近似训练目标是最小化数据和噪声样本的负对数后验分布。<span
class="math inline">\(P_d\)</span>表示实际的数据分布。对于<span
class="math inline">\(P_d\)</span>，v 是与 query
相对应的特征，即正样本；而对于 <span
class="math inline">\(P_n\)</span>，<span
class="math inline">\(v^{&#39;}\)</span>是从另一张图像中根据噪声分布
<span class="math inline">\(P_n\)</span>
随机抽样得到的特征，即负样本。v和 v′
都是从非参数化的内存库中抽样得到的。 <span class="math display">\[
J_{N C E}(\boldsymbol{\theta})=-E_{P_d}[\log h(i, \mathbf{v})]  -m \cdot
E_{P_n}\left[\log \left(1-h\left(i,
\mathbf{v}^{\prime}\right)\right)\right]
\]</span> 将Zi其视为常数，并通过蒙特卡罗逼近来估计其值，其中 <span
class="math inline">\({j_k}\)</span> 是字典的抽样子集。得到最终的NCE
loss为 <span class="math display">\[
Z \simeq Z_i \simeq n E_j\left[\exp \left(\mathbf{v}_j^T \mathbf{f}_i /
\tau\right)\right]=\frac{n}{m} \sum_{k=1}^m \exp
\left(\mathbf{v}_{j_k}^T \mathbf{f}_i / \tau\right) \\
J_\theta=-\sum_{w_i \in V}\left[\log \frac{\exp \left(\mathbf{v}^T
\mathbf{f}_i / \tau\right)}{\exp \left(\mathbf{v}^T \mathbf{f}_i /
\tau\right)+m P_n(i)}+\sum_{j=1}^k \log \left(1-\frac{\exp
\left(\mathbf{v}_{j_k}^T \mathbf{f}_i / \tau\right)}{\exp
\left(\mathbf{v}_{j_k}^T \mathbf{f}_i / \tau\right)+m
P_n(i)}\right)\right]
\]</span></p>
<p><strong>InfoNCE loss</strong></p>
<p>Info NCE
loss是NCE的一个简单变体，它认为如果你只把问题看作是一个二分类，只有数据样本和噪声样本的话，可能对模型学习不友好，因为很多噪声样本可能本就不是一个类，因此还是把它看成一个多分类问题比较合理
基本思想：一个编码好的query（一个特征），以及一系列编码好的样本k0,k1,k2...看作是字典里的key。假设字典里只有一个key是跟query是匹配的，那么q和k+就互为正样本对，其余的key为q的负样本。当query和唯一的正样本k+相似，并且和其他所有负样本key都不相似的时候，这个loss的值应该比较低。反之，如果query和k+不相似，或者q和其他负样本的key相似了，那么loss就应该大，从而惩罚模型，促使模型进行参数更新。
<span class="math display">\[
\mathcal{L}_q=-\log \frac{\exp \left(q \cdot k_{+} /
\tau\right)}{\sum_{i=0}^K \exp \left(q \cdot k_i / \tau\right)}
\]</span></p>
<ul>
<li><span class="math inline">\(q \cdot
k\)</span>可以看成模型输出的logit，这样的话InfoNCE
loss在形式上和cross-entropy loss相同。</li>
<li><span
class="math inline">\(\tau\)</span>是超参数——温度，一般用来控制分布的形状，如果<span
class="math inline">\(\tau\)</span>变大，分布更平滑，如果<span
class="math inline">\(\tau\)</span>变小，分布更集中。所以，如果<span
class="math inline">\(\tau\)</span>设的值较大，则对比损失对所有负样本一视同仁，模型学习没有轻重之分。如果<span
class="math inline">\(\tau\)</span>设置过小，则会使模型只关注那些特别困难的样本，导致模型很难收敛，或者学好的特征不易泛化。</li>
<li>K时负样本的数量，求和在一个正样本和 K
个负样本上。直观地说，这种损失是基于 (K+1) 路 softmax
的分类器的对数损失，该分类器试图将 q 分为 k+类。</li>
</ul>
<h3 id="代理任务">代理任务</h3>
<p>设计巧妙的代理任务 pretext task，人为设立一些规则 ——
定义哪些图片相似、哪些图片不相似，为自监督学习提供监督信号，从而自监督训练。代理任务的目标就是生成一个自监督的信号，从而去充当ground
truth这个标签信号，利用目标函数去衡量二者之间的差异，使模型学的更好。</p>
<p>instance discrimination（个体判别）：一个无标注的数据集，n
张图片，<span
class="math inline">\(x_1,x_2,...x_n\)</span>。随机选取一张图片，做
transformation。以<span
class="math inline">\(x_i\)</span>图片为例，<span
class="math inline">\(x_i\)</span>随机裁剪 + 数据增广 得到<span
class="math inline">\(x_i^1\)</span>, <span
class="math inline">\(x_i^2\)</span>（看起来和<span
class="math inline">\(x_i\)</span>有区别的 2 张照片，<span
class="math inline">\(x_i\)</span>的正样本），数据集中的其它图片<span
class="math inline">\(x_j, j ≠ i\)</span>是<span
class="math inline">\(x_i\)</span>的负样本。</p>
<p><strong>代理任务生成</strong></p>
<ul>
<li>denoising auto-encoders 重建整张图</li>
<li>context auto-encoders 重建某个 patch</li>
<li>cross-channel auto-encoders (colorization)
给图片上色当自监督信号</li>
<li>pseudo-labels 图片生成伪标签</li>
<li>exemplar image
给同一张图片做不同的数据增广，它们都属于同一个类。</li>
<li>patch ordering 九宫格方法：打乱了以后预测 patch 的顺序, or
随机选一个 patch 预测方位 eight positions</li>
<li>利用视频的顺序做 tracking</li>
<li>做聚类的方法 clustering features</li>
</ul>
<p><strong>对比学习和代理任务的关系</strong></p>
<p>不同的代理任务可以和某种形式的对比学习的目标函数配对使用</p>
<ul>
<li>MoCo：论文里 instance discrimination 个体判别方法 examplar based
代理任务很相关</li>
<li>CPC(contrastive predictive coding)：用上下文信息预测未来 context
auto-encoding 上下文自编码</li>
<li>CMC(contrastive multiview coding)： 利用一个物体的不同视角做对比
colorization 图片上色（同一个图片的 2 个视角：黑白 和 彩色）</li>
</ul>
<h2 id="对比学习论文梳理">对比学习论文梳理</h2>
<h3 id="第一阶段百花齐放">第一阶段：百花齐放</h3>
<h4 id="instdiscinstance-discrimination">InstDisc（instance
discrimination）</h4>
<p>论文：Unsupervised Feature Learning via Non-Parametric Instance
Discrimination <a
target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Unsupervised_Feature_Learning_CVPR_2018_paper.html">paper</a></p>
<h5 id="基本思想">基本思想</h5>
<p>在有监督学习中一些图片聚集在一起的原因并不是因为它们有相似的语义标签，而是因为这些照片本身十分相似，某一些
object 就是很相似，它们跟另外一些 object
的呢就是不相似，所以才会导致有些分类分数都很高，而有些分数非常低。</p>
<p>根据这个观察提出了个体判别任务：无监督的学习方式就是把按照类别走的有监督信号推到了极致，现在把每一个
instance都看成是一个类别，也就是每一张图片都看作是一个类别，目标是能学一种特征能把每一个图片都区分开来</p>
<h5 id="主体方法">主体方法</h5>
<figure>
<img
src="https://raw.githubusercontent.com/ultramarine-indigo/ultramarine-indigo.github.io/main/imgs/InstDisc%20framework.png"
alt="InstDisc framework" />
<figcaption aria-hidden="true">InstDisc framework</figcaption>
</figure>
<ul>
<li>通过一个CNN backbone把所有的图片都编码成一个特征，投影到 128
维空间并进行 L2 归一化。通过个体
判别来学习最佳特征嵌入，这些特征在最后的特征空间里能够尽可能的分开，因为对于个体判别任务来说每个图片都是自己的类，所以说每个图片都应该和别的图片尽量的分开。</li>
<li>使用对比学习训练模型。正样本就是这个图片本身（可能经过一些数据增强），负样本就是从
memory bank 里随机地抽取得到，利用NCE loss
计算对比学习的目标函数。一旦更新完这个网络，就可以把当前
batch里的数据样本所对应的那些特征，在 memory bank 里更换掉，这样 memory
bank 就得到了更新。通过不停的去更新这个编码 t 和 memory
bank，使最后学到这个特征尽可能的有区分性</li>
<li>memory bank：存放所有图片的 encoded
特征，也就是一个字典（ImageNet数据集有128万的图片，也就是说memory
bank里要存128万行，也就意味着每个特征的维度不能太高，否则存储代价太大了，本文用的是128维）</li>
</ul>
<h5 id="实验细节">实验细节</h5>
<p>proximal regularization：给模型的训练加了一个约束，从而能让 memory
bank 里的那些特征进行动量式的更新。</p>
<h5 id="总结">总结</h5>
<p>Inst Disc
这篇论文也是一个里程碑式的工作。不仅提出了个体判别这个代理任务，而且用这个代理任务和
NCE
loss做对比学习，从而取得了不错的无监督表征学习的结果，同时它还提出了用别的数据结构存储这种大量的负样本，以及如何对特征进行动量的更新，所以真的是对后来对比学习的工作起到了至关重要的推进作用</p>
<h4 id="cpc">CPC</h4>
<p>论文：Representation Learning with Contrastive Predictive Coding <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1807.03748">paper</a></p>
<h5 id="基本思想-1">基本思想</h5>
<p>利用预测的方式实现对比学习：对于一个序列，通过当前时刻以前的一段输入得到的特征去预测未来时刻的特征。这相当于利用之前时刻得到的预测特征是
query；正样本是真正未来时刻通过编码器得到的特征；负样本的定义其实很广泛，比如可以任意选取其他序列通过这个编码器得到输出，它都应该跟预测是不相似的。</p>
<h5 id="主体方法-1">主体方法</h5>
<figure>
<img
src="https://raw.githubusercontent.com/ultramarine-indigo/ultramarine-indigo.github.io/main/imgs/CPC%20overview.png"
alt="CPC framework-algorithm" />
<figcaption aria-hidden="true">CPC framework-algorithm</figcaption>
</figure>
<p>假设有一个输入
x（一个持续的序列），t表示当前时刻，t-i表示过去的时刻，t+i表示未来的时刻。把之前时刻的输入全都扔给一个编码器，这个编码器就会返回一些特征，然后把这些特征喂给一个自回归的模型（gar，auto
regressive），一般常见的自回归模型，就是 RNN 或者
LSTM的模型，所以每一步最后的输出，就会得到图中红色的方块（ct，context
representation，代表上下文的一个特征表示），如果这个上下文的特征表示足够好（它真的包含了当前和之前所有的这些信息），那它应该可以做出一些合理的预测，所以就可以用ct预测未来时刻的这个zt
+1、zt +
2（未来时刻的特征输出）。计算预测特征和未来真实特征之间的infoNCE
loss，从而训练模型。</p>
<p>CPC模型非常灵活。如果把输入序列换成一个句子，也可以说用前面的单词来预测后面的单词的特征输出；如果把这个序列想象成一个图片的patch块从左上到右下形成一个序列，就可以用上半部分的图片特征去预测后半部分的图片特征，</p>
<h4 id="cmc">CMC</h4>
<p>论文：contrastive multiview coding <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1906.05849">paper</a></p>
<h5 id="基本思想-2">基本思想</h5>
<p>人观察这个世界是通过很多个传感器，比如说眼睛或者耳朵都充当着不同的传感器来给大脑提供不同的信号。每一个视角都是带有噪声的，而且有可能是不完整的，但是最重要的那些信息其实是在所有的这些视角中间共享，比如说基础的物理定律、几何形状或者说它们的语音信息都是共享的。</p>
<p>基于这个现象，本文就提出学一个非常强大的特征，其具有视角的不变性（不管看哪个视角，不论是看到了一只狗，还是听到了狗叫声，都能判断出这是个狗）。因此
CMC
工作的目的就是去增大互信息（所有的视角之间的互信息），试图能学到一种特征能够抓住所有视角下的关键的因素。</p>
<h5 id="主体方法-2">主体方法</h5>
<p>虽然这些不同的输入来自于不同的传感器或者说不同的模态，但是所有的这些输入其实对应的都是一个物体，那它们就应该互为正样本。也就是说，在特征空间中，同一物体来自不同视角（模态）的样本特征在这个特征空间里就应该非常的接近。而不同物体的样本，不论是来自不同视角（模态）还是同一视角（模态），他们的特征就彼此远离。</p>
<h5 id="总结-1">总结</h5>
<ul>
<li>CMC
是第一个或者说比较早的工作去做这种多视角的对比学习，它不仅证明了对比学习的灵活性，而且证明了这种多视角、多模态的这种可行性。之后open
AI
很快推出了clip模型：也就是说如果有一个图片，还有一个描述这个图片的文本，那这个图像和文本就可以当成是一个正样本对，就可以拿来做多模态的对比学习</li>
<li>CMC
作者用对比学习的思想做了一篇蒸馏的工作：不论用什么网络，不论这个网络是好是坏是大是小，只要你的输入是同一张图片，那得到的这个特征就应该尽可能的类似，也就意味着想让
teacher 模型的输出跟 student 模型的输出尽可能的相似，它就通过这种方式把
teacher和student做成了一个正样本对，从而可以做对比学习</li>
<li>局限性：当处理不同的视角或者说不同的模态时候，可能需要不同的编码器，因为不同的输入可能长得很不一样，这就有可能会导致使用几个视角，有可能就得配几个编码器，在训练的时候这个计算代价就有点高（比如说在
clip 这篇论文里，它的文本端就是用一个大型的语言模型，比如说
bert，它的图像端就是用一个 vit，就需要有两个编码器）。</li>
</ul>
<h4 id="总结-2">总结</h4>
<ul>
<li>使用的代理任务不同（个体判别、预测未来、多视角多模态）</li>
<li>目标函数不同（NCE、InfoNCE、NCE的其他变体）</li>
<li>模型不同（InvaSpread仅用一个编码器、InstDisc用一个编码器和memory
bank、CPC是一个编码器和一个自回归模型、CMC有两个甚至多个编码器）</li>
</ul>
<h3 id="第二阶段模型细节统一">第二阶段：模型细节统一</h3>
<h4 id="moco">MoCo</h4>
<p>论文：Momentum Contrast for Unsupervised Visual Representation
Learning <a
target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/html/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.html">paper</a></p>
<p><strong>Momentum 动量</strong>：(指数)加权移动平均值 <span
class="math display">\[
y_t=m * y_{t - 1} + (1 - m) * x_t
\]</span> <span
class="math inline">\(y_t\)</span>为当前时刻的输出；<span
class="math inline">\(y_{t - 1}\)</span>为上一个时刻的输出；<span
class="math inline">\(x_t\)</span>为当前时刻的输入。m为动量的超参数，m
趋近于1，<span
class="math inline">\(y_t\)</span>改变缓慢，当前时刻的输入<span
class="math inline">\(x_t\)</span>没什么影响；m 趋近于
0，更多依赖于当前时刻的输入。</p>
<h5 id="基本思想-3">基本思想</h5>
<p>将对比学习看成一个dictionary
look-up（字典查询任务），构建一个大的、一致的字典，有利于无监督的对比学习训练。因此，MoCo
构建了一个a dynamic dictionary with a queue and a moving-averaged
encoder 动态字典，从而可以很好的无监督学习视觉特征。</p>
<ul>
<li><strong>一个队列</strong>：队列中的样本无需梯度回传，可以放很多负样本，让字典变得很大</li>
<li><strong>一个移动平均的编码器</strong>：利用动量的特性，缓慢的更新一个编码器，从而让中间学习到的字典中的特征尽可能保持一致。</li>
</ul>
<h5 id="研究动机">研究动机</h5>
<p>无监督学习在 CV 不成功的原因：CV和NLP具有不同的原始信号空间。NLP
的离散单词更具语义性；CV的连续、高维信号不好构建字典。</p>
<ul>
<li>NLP 原始信号是离散的，由词、词根、词缀构成，容易构建 tokenized
dictionaries（tokenized: 把一个词对应成某一个特征）。通过把字典的 key
认为是一个类别，就有类似标签的信息帮助学习，从而转换为有监督的范式。因此，NLP
无监督学习很容易建模，建好的模型也好优化。</li>
<li>CV
原始信号是连续的、高维的，不像单词具有浓缩好的、简洁的语义信息，不适合构建一个字典。如果没有字典，无监督学习很难建模。</li>
</ul>
<p>对比学习可以被看成构建动态字典的查询任务：字典中的key从数据（例如图像或补丁）中采样，并由编码器网络表示。无监督学习训练编码器执行字典查找：编码的query应该与其匹配key相似，与其他key不同。</p>
<ul>
<li>large：从连续高维空间做更多的采样。字典 key
越多，表示的视觉信息越丰富，匹配时更容易找到具有区分性的本质特征。如果字典小、key
少，模型可能学到 shortcut 捷径，不能泛化。</li>
<li>consistent：字典里的 key (k0, k1, k2, ......, kN) 应该由相同的 or
相似的编码器生成。如果字典的 key 是由不同的编码器得到的，query q
做字典查询时，很有可能 找到和 query 使用同一个 or 相似编码器生成的
key，而不是语义相似的 key，另一种形式的 shortcut solution。</li>
</ul>
<h5 id="主体方法-3">主体方法</h5>
<figure>
<img
src="https://raw.githubusercontent.com/ultramarine-indigo/ultramarine-indigo.github.io/main/imgs/MoCo%20framework-algorithm.png"
alt="MoCo framework-algorithm" />
<figcaption aria-hidden="true">MoCo framework-algorithm</figcaption>
</figure>
<p><strong>contrastive learning as dictionary look-up</strong></p>
<p>一个编码查询 q 和一组编码样本 {k0, k1, k2,...}
是字典键。假设字典中有一个键（表示为 k+）与q
匹配。对比损失是一个函数，当 q 与其正键 k+
相似并且与所有其他键不同时，其值较低（被认为是 q
的负键）。通过点积测量的相似性，MoCo使用了InfoNCE作为损失函数。</p>
<p><strong>Momentum Constrast</strong></p>
<ul>
<li><p><strong>Dictionary as a queue</strong></p>
<p>将字典维护为数据样本队列，字典总是表示所有数据的采样子集。重用已经编码好的key（即之间mini
batch得到的），让字典的大小和模型每次做前向传播的 batch size
的大小分开。每次做 iteration 更新，当前 mini-batch 入队，最早进入队列的
mini-batch 出队，并不需要更新字典中所有 key 元素的值</p></li>
<li><p><strong>momentum encoder</strong></p>
<p>为了保持字典中key的一致性，使用动量更新key encoder。 <span
class="math display">\[
\theta_k = m * \theta_{k-1} + (1-m) * \theta_q
\]</span></p>
<ul>
<li><span class="math inline">\(\theta_k\)</span>当前时刻key
encoder；<span class="math inline">\(\theta_{k-1}\)</span>前一时刻key
encoder；<span class="math inline">\(\theta_q\)</span>是当前时刻query
encoder，只有<span
class="math inline">\(\theta_q\)</span>通过反向传播更新参数。</li>
<li>m为动量参数，m 设置较大时，<span
class="math inline">\(\theta_k\)</span>的更新缓慢，不过多的依赖于<span
class="math inline">\(\theta_q\)</span>当前时刻的编码器，即不随着当前时刻的编码器快速改变，尽可能保证字典里的
key 都是由相似的编码器生成的特征。</li>
</ul></li>
</ul>
<h5 id="总结-3">总结</h5>
<figure>
<img
src="https://raw.githubusercontent.com/ultramarine-indigo/ultramarine-indigo.github.io/main/imgs/MoCo-comparison%20of%20three%20contrastive%20mechanisms.png"
alt="MoCo-comparison of three contrastive mechanisms" />
<figcaption aria-hidden="true">MoCo-comparison of three contrastive
mechanisms</figcaption>
</figure>
<ul>
<li>end to
end：编码器都是可以通过梯度回传来更新模型参数的。因为模型的正负样本都是从同一个mini-batch里来的，也就是xq和xk都是从同一个batch中来的，做一次forward就能得到所有样本的特征，而且这些样本是高度一致的，因为都是来自一个编码器。端到端学习的优点在于编码器是可以实时更新的，所以导致它字典里的那些key的一致性是非常高的，但是它的缺点在于因为它的字典大小（就是batch-size的大小），导致这个字典不能设置的过大，否则硬件内存吃不消，而且大的batch-size的优化也是一个难点，如果处理不得当，模型是很难收敛的。</li>
<li>memory
bank：只有一个编码器，也就是query的编码器是可以通过梯度回传来进行更新学习的，但是对于字典中的key是没有一个单独的编码器的。memory
bank把整个数据集的特征都存到了一起，每次模型做训练的时候，只需要从memory
bank中随机抽样很多的key出来当作字典就可以了，就相当于上述构建字典的操作都是在线下执行的，所以不用担心硬件内存的问题，也就是字典可以抽样抽的很大。但是memory
bank的做法在特征一致性上就处理的不是很好，每一个epoch都对抽样的特征进行更新，所以memory
bank里的特征都是在不同时刻的编码器得到的，而且这些编码器都是通过梯度回传来进行快速更新的，这也就意味着这里得到的特征都缺乏一致性。</li>
<li>MoCo采用队列的形式去实现字典，从而使得它不像端到端的学习一样受限于batch-size的大小。同时为了提高字典中特征的一致性，MoCo使用了动量编码器。既简单又高效，而且扩展性还好，能同时提供一个又大而且又一致的字典，从而进行对比学习</li>
</ul>
<h4 id="simclr">SimCLR</h4>
<p>论文：A Simple Framework for Contrastive Learning of Visual
Representations <a
target="_blank" rel="noopener" href="http://proceedings.mlr.press/v119/chen20j.html">paper</a></p>
<h5 id="主要思路">主要思路</h5>
<p>SimCLR
的优越之处就是在于它的成功不是由任何一个单一的设计决定，而是把所有的这些技术结合起来而得到的一个结果。</p>
<ul>
<li>用了更多的数据增强，发现对比学习真的是需要很强的数据增强的技术。SimCLR
使用的数据增强方法，从最开始的原始图片，到裁剪，改变色彩，旋转，cut
out，还有高斯的噪声和高斯
blur，以及sobel滤波器，最后发现最有效的两个数据增强就是随机的裁剪和随机的这种色彩变换。</li>
<li>提出了在模型后面加入一个projector降维，即g函数（一个可以学习的分线性的变换，就是一个
mlp层）</li>
<li>使用了更大的batch size
，而且训练的时间更久，并发现这两个策略都能让网络学到的特征变得更好</li>
</ul>
<h5 id="主体方法-4">主体方法</h5>
<figure>
<img
src="https://raw.githubusercontent.com/ultramarine-indigo/ultramarine-indigo.github.io/main/imgs/SimCLR%20framework-algorithm.png"
alt="SimCLR-framework" />
<figcaption aria-hidden="true">SimCLR-framework</figcaption>
</figure>
<ul>
<li>一个mini-batch的图片为
x，对这个mini-batch里的所有图片做不同的数据增强就会得到x
i和xj，同一个图片延伸得到的两个图片就是正样本，也就是说如果batch
size是n的话，正样本个数就是n，负样本的个数就是这个 batch size
剩下所有的样本以及它们数据增强过后的样本，也就是 2n-1。</li>
<li>正负样本通过编码器f对它进行编码，得到的特征h。之后通过一个projector降维，也就是上图中的g函数，它就是一个mlp层（只有一个全连接层，后面跟一个
relu
的激活函数）。即数据样本经过编码器得到一个特征之后再做一个非线性变化，就得到了最后去做对比学习的那个特征z，一般这个特征z维度会小一点，为了跟之前的工作保持一致性，本文为128维。</li>
<li>最后要衡量一下正样本之间是不是能达到最大的一致性，它采用的是normalized
temperature-scaled的交叉熵函数。normalized就是说在这个特征后面进行了 L2
归一化，temperature-scaled 就是说在这个 loss
乘了个tao，所以说其实这个loss跟之前说的infoNCE loss也是非常接近的。</li>
<li>projector降维，即g函数只有在训练的时候才用，而在做下游任务的时候，是把g函数去掉了，还是只用h这个特征去做下游任务，加上这个g函数只是为了能让模型训练的更好。</li>
</ul>
<h5 id="总结-4">总结</h5>
<p>SimCLR这篇文章中提出来的很多技术都对后续的工作产生了长远的影响力，比如说在编码器之后加这么一个projector
降维，在之后的MoCo
v2、BYOL这些工作里全都有使用；它使用的数据增强的策略在之后的工作里也是被广泛的使用；它使用lars这个优化器去做大batch
size的这个模型训练，之后BYOL
也采用了同样的策略。总之SimCLR真的是够简单有效，为后续的很多研究铺平了道路</p>
<h4 id="swav">SwAV</h4>
<p>论文：Unsupervised Learning of Visual Features by Contrasting Cluster
Assignment <a
target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2020/hash/70feb62b69f16e0238f741fab228fec2-Abstract.html">paper</a></p>
<h5 id="基本思路">基本思路</h5>
<p>给定同样一张图片，如果生成不同的视角，不同的 views
的话，希望可以用一个视角得到的特征去预测另外一个视角得到的特征。具体的做法就是把对比学习和聚类的方法合在了一起。</p>
<ul>
<li>聚类方法也是一种无监督的特征表示学习方式，而且呢它也是希望相似的物体都聚集在某一个聚类中心附近，不相似的物体尽量推到别的聚类中心，所以跟对比学习的目标和做法都比较接近。</li>
<li>直接拿所有图片的特征和特征做对比有点原始而且费资源，因为所有的图片都是自己的类，类别太多。SwAV
提出可以去跟聚类的中心对比，聚类中心也就是特征空间中的特征向量。此外，聚类中心是有明确的语意含义的，如果之前只是随机抽样抽取负样本去做对比的话，那些负样本有的可能还是正样本的，而且有的时候抽出来的负样本类别也不均衡，所以不如使用聚类中心有效。</li>
</ul>
<h5 id="主体方法-5">主体方法</h5>
<figure>
<img
src="https://raw.githubusercontent.com/ultramarine-indigo/ultramarine-indigo.github.io/main/imgs/SWaV-framework.png"
alt="SWaV-framework" />
<figcaption aria-hidden="true">SWaV-framework</figcaption>
</figure>
<ul>
<li>一个mini-batch的图片，做两次数据增强得到x1、x2，分别通过编码器得到特征z1、z2。然后，通过clustering让特征z和聚类中心（prototype）
c生成目标q，q1、q2就相当于ground truth。</li>
<li>Swapped prediction：由于x1、x2是正样本对，那z1 和
z2的特征就应该很相似。两个特征非常相似（含有等量的信息）的时候，是可以互相去做预测的。也就是说，用z1这个特征去跟c去做点乘，是可以去预测q2；反之亦然，z2和这个c去做点乘也可以预测q1，所以说点乘之后的结果就是预测，而ground
truth就是之前按照clustering
而得到的q1和q2。所以通过这种换位预测的方法，SwAV可以对模型进行训练。</li>
</ul>
<h5 id="训练细节multi-crop">训练细节——multi crop</h5>
<ul>
<li>之前的那些对比的学习方法都是用的两个crop，也就是说一个正样本对x1、x2两个图片。假设有一个图片，先把它resize
到256*256，然后随机crop两个224*224的图片当成
x1、x2，因为这两张图片都非常大，所以它们重叠的区域也非常多，于是它们就应该代表一个正样本对。</li>
<li>用大的crop抓住的是整个场景的特征，如果更想学习这些局部物体的特征，最好能多个
crop，去图片里crop一些区域，这样就能关注到一些局部的物体了。但是增加crop，也就是说增加view，会增加模型的计算复杂度，因为相当于使用了更多的正样本。为了不增加太多的这个计算成本，把这个crop变得小一点，变成160
，也就是说取2个160的crop去争取学全局的特征。然后为了增加正样本的数量，学一些局部的特征，再去随机选4个小一点crop，大小是96*96。这样的话，就意味着现在有6个视角了，所以正样本的数量增多了。</li>
</ul>
<h4 id="总结-5">总结</h4>
<p>模型细节已经趋于统一了</p>
<ul>
<li>目标函数都是infoNCE或者其变体</li>
<li>模型都是用一个编码器后面加一个projection head</li>
<li>更强的数据增强</li>
<li>动量编码器</li>
</ul>
<h3 id="第三阶段不用负样本">第三阶段：不用负样本</h3>
<h4 id="byol">BYOL</h4>
<p>论文：Bootstrap your own latent: A new approach to self-supervised
Learning <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2006.07733">paper</a></p>
<h5 id="基本思想-4">基本思想</h5>
<p>在对比学习中，使用负样本是一种约束，如果在计算目标函数只有正样本，则目的只有让相似样本的特征尽可能相似。此时模型会存在一个捷径解，即对于不同的输入，都返回相同的输出，则loss为0，但并没有学习到好的特征表示。</p>
<p>BYOL 有两个神经网络：online和target，它们相互影响、相互学习。BYOL
并不使用负样本，而是利用图像的增强视图，训练 online 来预测 target
在不同增强视图下对同一图像的表示。同时，我们用online的缓慢移动平均值（动量）来更新target。</p>
<h5 id="主体方法-6">主体方法</h5>
<figure>
<img
src="https://raw.githubusercontent.com/ultramarine-indigo/ultramarine-indigo.github.io/main/imgs/BYOL-framework.png"
alt="BYOL framework" />
<figcaption aria-hidden="true">BYOL framework</figcaption>
</figure>
<p>将对比学习中的匹配问题转化为预测的问题，即通过自己一个视角的特征去预测自己另一个视角的特征。</p>
<ul>
<li>一个mini-batch的图片，做两次数据增强得到<span
class="math inline">\(v\)</span>，<span
class="math inline">\(v^{&#39;}\)</span>，分别通过编码器<span
class="math inline">\(f_\theta\)</span>，<span
class="math inline">\(f_\xi\)</span>得到特征<span
class="math inline">\(y_\theta\)</span>，<span
class="math inline">\(y_\xi^{&#39;}\)</span>。然后对得到的特征通过一个projection进行降维，得到征<span
class="math inline">\(z_\theta\)</span>，<span
class="math inline">\(z_\xi^{&#39;}\)</span>。降维后的特征<span
class="math inline">\(z_\theta\)</span>通过一个MLP进行predict，得到预测特征<span
class="math inline">\(q_\theta(z_\theta)\)</span>，希望预测特征<span
class="math inline">\(q_\theta(z_\theta)\)</span>和下面的特征<span
class="math inline">\(z_\xi^{&#39;}\)</span>尽可能一致，使用MSE
loss。</li>
<li>上面的<span
class="math inline">\(f_\theta\)</span>相当于query编码器，下面的<span
class="math inline">\(f_\xi\)</span>相当于key的编码器。key编码器（online网络）参数都是query编码器（target
网络）参数的动量更新。</li>
</ul>
<h4 id="simsiam">SimSiam</h4>
<p>论文：Exploring Simple Siamese Representation Learning <a
target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2021/html/Chen_Exploring_Simple_Siamese_Representation_Learning_CVPR_2021_paper.html">paper</a></p>
<h5 id="基本思想-5">基本思想</h5>
<p>孪生网络已成为最近各种无监督视觉表征学习模型中的常见结构。在模型不坍塌的前提下，这些模型最大化一个图像的两个增强视角图像之间的相似性。SimSiam发现简单的孪生网络也能学习有意义的表征，在不使用负样本对，大batch
size，动量编码器下。模型依然能够学习到好的表征，同时，停止梯度操作在防止坍缩方面发挥了重要作用，使得SimSiam
的行为就像是在两组变量之间交替优化。</p>
<p>Siamese架构可能是相关方法取得共同成功的一个关键原因。Siamese网络可以自然地引入归纳偏见以建模不变性，“不变性”意味着相同概念的两个观察结果应该产生相同的输出。类似于卷积，它通过权重共享成功地引入了一种归纳偏见，用于建模平移不变性，具有权重共享的Siamese网络可以模拟对更复杂的变换（例如增强）的不变性。</p>
<h5 id="主体方法-7">主体方法</h5>
<p><img src="https://raw.githubusercontent.com/ultramarine-indigo/ultramarine-indigo.github.io/main/imgs/SimSiam-framework.png" alt="SimSiam-framework"  /></p>
<p>一个mini-batch的图片，做两次数据增强得到x1、x2，分别通过编码器得到特征z1、z2。然后利用一个MLP去进行预测，即z1经过prediction得到p1去预测z2，z2经过prediction得到p2去预测z1。最后计算p和z之间的MSE
loss来完成模型训练。</p>
<p><strong>对比总结</strong></p>
<p><img src="https://raw.githubusercontent.com/ultramarine-indigo/ultramarine-indigo.github.io/main/imgs/Comparison on Siamese architectures.png" alt="Comparison on Siamese architectures" style="zoom:67%;" /></p>
<ul>
<li>SimCLR两边都有梯度回传，做的是一个对比任务</li>
<li>SwAV也是做对比任务，但并没有跟负样本比，而是和聚类中心比，聚类中心是通过SK算法得到的</li>
<li>BYOL不是对比任务，加了predictor变成一个预测任务，用左边去预测右边，同时他们还使用了动量编码器</li>
<li>SimSiam的整个左边和BYOL一模一样，不同的是右边没有用动量编码器而是共享参数了</li>
</ul>
<p><strong>模型假设</strong>：之所以能够成功训练，不会模型坍塌，是因为有stop
gradient的存在。而stop
gradient的存在，使得SimSiam可以看成一个EM(expectation-maximization)算法。因为stop
gradient的存在，一个训练过程（或者说模型参数）被人为分成了两个部分，相当于在解决两个子问题，模型的更新也是在交替进行。所以，可以看成一个K-means聚类，K-means本身就分为两步，先把所有点分配给聚类中心，然后再利用分配后的结构更新聚类中心。</p>
<h3 id="对比学习模型总结">对比学习模型总结</h3>
<h4 id="第一阶段百花齐放-1">第一阶段：百花齐放</h4>
<ul>
<li>InstDisc：提出了个体判别任务，提出用一个memory
bank的外部数据结构去存储负样本，从而得到一个又大又一致的字典去做对比学习</li>
<li>InvaSpread：不用外部结构的另一条路，端到端学习。只用一个编码器，从而可以端到端学习，但因为受限于batch
size太小，所以性能不够好。</li>
<li>CPC：提出了infoNCEloss，CPC是一个预测型的代理任务，不仅可以做图像，还可以做音频、视频、文字和强化学习，是一个非常全能的结构。</li>
<li>CMC：把两个视角的任务扩展到了多个视角，给接下来的多视角或者多模态的对比学习打下了铺垫。</li>
</ul>
<h4 id="第二阶段模型细节统一-1">第二阶段：模型细节统一</h4>
<ul>
<li>MoCo：可以看成是InstDisc的延伸工作，把memory
bank变成一个队列，把动量更新特征变成了动量更新编码器，从而能预训练一个很好的模型。moco也是第一个在很多视觉的下游任务上，让一个无监督预训练的模型比有监督预训练模型表现好的方法。属于使用外部数据结构的。</li>
<li>SimCLR：是端到端的延伸性工作，和InvaSpread很像，但是用了很多的技术：加大batch
size、用了更多数据增强、加了一个Projection
head、训练更长时间。所有的技术堆起来，让SimCLR在imagenet上取得了非常好的结果。</li>
<li>CPCv2，也把这些技术用了一遍，直接比CPC在imagenet上的结果高了三十几个点。</li>
<li>CMC：把这些都分析了一下，提出了一个infomin的原则：两个视角之间的互信息要不多不少才是最好的。</li>
<li>SwAV：将聚类学习和对比学习结合起来的一个工作，取得了不错的效果，这个不错的结果主要来自于它提出的multicrop的技术，如果没有这个技术就和MoCo或者SimCLR结果差不多。</li>
</ul>
<h4
id="第三阶段不用负样本的对比学习">第三阶段：不用负样本的对比学习</h4>
<ul>
<li>BYOL：提出处理负样本太麻烦，不要负样本了，不用负样本做对比了。把对比任务变成预测任务，自己跟自己学就行了。目标函数也很简单就是MSEloss就训练出来了</li>
<li>SimSiam：总结了之前的工作，化繁为简提出来一个很简单的孪生网络的学习方法。SimSiam既不需要大的batch
size，也不需要动量编码器，也不需要负样本，照样能取得不错的结果。SImSiam提出的假设是stop
gradient这个操作至关重要，因为有这个操作的存在，SimSiam可以看作是一种EM算法，通过逐步更新的方式避免模型坍塌。</li>
<li>barlos
twins：更换了一个目标函数，把之前大家做的对比和预测变成了两个矩阵之间比相似性。</li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" rel="tag"># 对比学习</a>
              <a href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" rel="tag"># 无监督学习</a>
              <a href="/tags/NCE-loss/" rel="tag"># NCE loss</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/06/18/linux%E5%86%85%E6%A0%B8%E6%A8%A1%E5%9D%97%E8%AE%BE%E8%AE%A1/" rel="prev" title="linux内核模块设计">
                  <i class="fa fa-angle-left"></i> linux内核模块设计
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/09/04/speech-model%E8%AF%AD%E9%9F%B3%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93/" rel="next" title="speech model语音模型总结">
                  speech model语音模型总结 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2023</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Ultramarine</span>
  </div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script size="300" alpha="0.6" zIndex="-1" src="https://cdnjs.cloudflare.com/ajax/libs/ribbon.js/1.0.2/ribbon.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">false</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
